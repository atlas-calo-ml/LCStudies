{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopoCluster Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple, stripped-down notebook for training networks. I've removed most of the models that are present in `TopoClusterRegressionRewrite.ipynb`, as well as most of the plots (I find that the multitude of plots makes things a bit cumbersome and hard to navigate -- I'll see if I can change the way they are displayed later on).\n",
    "\n",
    "Here, we train a number of different models -- by only running certain cells, you can choose which to train and evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML fitting/loading/saving settings\n",
    "overwriteModel = False # If true, force training. If false, load the specified model if it already exists.\n",
    "\n",
    "finishTraining = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's choose our training data (and associated strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data choice\n",
    "# options are jet, pion, pion_reweighted\n",
    "\n",
    "# options are:\n",
    "#   - pion_legacy\n",
    "#   - pion\n",
    "#   - pion_reweighted\n",
    "\n",
    "source = 'pion'\n",
    "subdir = 'pion1' # name for subdir holding models/plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries.\n",
    "import sys, os, uuid, glob, pickle, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd # we will use some uproot/pandas interplay here.\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot as ur\n",
    "import ROOT as rt # used for plotting\n",
    "#import joblib as jl # for saving scalers\n",
    "from numba import jit\n",
    "\n",
    "# Import our resolution utilities\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util       as pu\n",
    "from util import ml_util         as mu\n",
    "from util import qol_util        as qu\n",
    "from util import io_util         as iu\n",
    "\n",
    "# Custom tensorflow.keras callbacks\n",
    "from util.keras.callbacks import GetCallbacks\n",
    "\n",
    "# Regression-specific utils\n",
    "from util.regression import data_util as rdu\n",
    "from util.regression import plot_util as rpu\n",
    "from util.regression import training_util as rtu\n",
    "\n",
    "rt.gStyle.SetOptStat(0)\n",
    "# use our custom dark style for plots\n",
    "plotstyle = qu.PlotStyle('dark')\n",
    "plotstyle.SetStyle() # still need to manually adjust legends, paves\n",
    "\n",
    "plotpath = path_prefix + 'regression/Plots/{}/'.format(subdir)\n",
    "modelpath = path_prefix + 'regression/Models/{}/'.format(subdir)\n",
    "paths = [plotpath, modelpath]\n",
    "for path in [plotpath, modelpath]:\n",
    "    try: os.makedirs(path)\n",
    "    except: pass\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "cell_shapes = {layers[i]:(len_eta[i],len_phi[i]) for i in range(len(layers))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy display names for each pion type\n",
    "pi_latex = {\n",
    "    'p0': '\\(\\pi^{0}\\)',\n",
    "    'pp': '\\(\\pi^{\\pm}\\)',\n",
    "}\n",
    "pi_text = {\n",
    "    'p0': 'pi0',\n",
    "    'pp': 'pi+/-'\n",
    "}\n",
    "\n",
    "# Plotting settings\n",
    "# xkcd -- turn this on for fun-looking (but marginally less useful) plots\n",
    "use_xkcd = False\n",
    "if(use_xkcd):\n",
    "    mode = 'light'\n",
    "    plt.xkcd(scale=.75,length=100,randomness=1)\n",
    "    \n",
    "# plotting style -- manages our color palette and object colors\n",
    "mode = 'dark' # for publications, use \"light\"\n",
    "plotstyle = qu.PlotStyle(mode)\n",
    "    \n",
    "# some matplotlib-specific stuff\n",
    "params = {'legend.fontsize': 13,\n",
    "          'axes.labelsize': 18}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Get the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me lay out some definitions, so it's clear as to what the data is.\n",
    "\n",
    "We have a number of different \"strategies\" (the `source` variable at the top). These correspond to different choices of training, validation and testing datasets.\n",
    "\n",
    "1. `pion`: We train and validate the network using our single pion data.\n",
    "\n",
    "2. `pion_legacy`: We train and validate using our old, noise-free single pion data.\n",
    "\n",
    "3. `pion_reweighted`: This is the same as `pion`, except that our training data is reweighted using a jet dataset (via their reco topo-cluster $p_T$ distributions), that corresponds with QCD dijet events.\n",
    "\n",
    "The validation performed for these networks is effectively being done on some \"holdout\" dataset from training -- it will by definition have similar kinematics, being drawn from the same set of events. The more interesting test -- how our energy regression performs in tandem with classification on our *unlabeled* jet dataset, will be handled in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(source == 'pion_legacy'):\n",
    "    inputpath = path_prefix+'data/pion_legacy/'\n",
    "    rootfiles = {\n",
    "        'p0':inputpath + 'pi0.root',\n",
    "        'pp':inputpath + 'pi[pm]*.root'\n",
    "    }\n",
    "    \n",
    "    branches = ['clusterE', 'clusterPt', 'clusterEta', 'cluster_ENG_CALIB_TOT']\n",
    "\n",
    "elif(source == 'pion' or source == 'pion_reweighted'):\n",
    "    inputpath=path_prefix+'data/pion/'\n",
    "    rootfiles = {        \n",
    "        'p0':inputpath + 'user.mswiatlo.900246.PG_singlepi0_logE0p2to2000.recon.ESD.e8312_e7400_s3170_r12383.images_v01.1_OutputStream/*.root',\n",
    "        'pp':inputpath + 'user.mswiatlo.900247.PG_singlepion_logE0p2to2000.recon.ESD.e8312_e7400_s3170_r12383.images_v01.1_OutputStream/*.root'\n",
    "    }\n",
    "    \n",
    "    branches = ['cluster_E', 'cluster_Pt', 'cluster_Eta', 'cluster_ENG_CALIB_TOT']\n",
    "\n",
    "else: assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "h5_name_suffix = 'tdata'\n",
    "h5_name = inputpath + h5_name_suffix\n",
    "\n",
    "pdata,pcells = mu.setupPionData(\n",
    "    rootfiles, \n",
    "    branches=branches, \n",
    "    layers=layers, \n",
    "    balance_data=True, \n",
    "    n_max = 300000,\n",
    "    verbose=True,\n",
    "    load=True,\n",
    "    save=True,\n",
    "    filename=h5_name,\n",
    "    match_distribution='cluster_ENG_CALIB_TOT',\n",
    "    match_binning = (20000,0.,2000.),\n",
    "    cut_distributions=['cluster_ENG_CALIB_TOT','clusterEta'],\n",
    "    cut_values = [.2, (-0.7,0.7)],\n",
    "    cut_types=['lower','window']\n",
    ")\n",
    "\n",
    "total = np.sum([len(x) for x in pdata.values()],dtype=int)\n",
    "for key,frame in pdata.items():\n",
    "    n = len(frame)\n",
    "    print(\"Number of {a:<7} events: {b:>10}\\t({c:.1f}%)\".format(a=pi_text[key], b = n, c = 100. * n / total))\n",
    "print(\"Total: {}\".format(total))\n",
    "\n",
    "# Create/get training/validation/testing indices.\n",
    "pdata = rdu.DataPrep(pdata,\n",
    "                     trainfrac=0.7,\n",
    "                     filename=h5_name\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining regression variables\n",
    "\n",
    "Beyond the information already present in the data, we may want to pre-compute some extra regression inputs.\n",
    "\n",
    "The difference between computing them here, on the whole dataset, versus doing it in batch as part of the network itself, is that we can also define some `scalers` based on these variables. This will allow us to scale them across the dataset, for example to get them into the interval of \\[0,1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1.\n",
    "b = 1.\n",
    "EnergyMapping = iu.LinLogMapping(b=b,m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform a test of our mapping to demonstrate its numerical stability\n",
    "# # TODO: Fix legend sizing, it is flaky (and needs re-running)\n",
    "# x=np.linspace(-10.,10.,10000)\n",
    "# b_vals=[1.,10.,1.0e14]\n",
    "# m_vals=[1.]\n",
    "# iu.MapStabilityTest(iu.LogMapping, b_vals=b_vals, m_vals=m_vals, x=x, ps=plotstyle, savedir=plotpath, legend_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our `LinLogMapping` is pretty numerically stable as long as $b \\gg m$. I think this is the expected behaviour, given the instabilities with logarithms that we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some regression vars.\n",
    "# Note that the mapping functions can probably be sped up with numba, but might throw errors during plotting above. Need to look into this.\n",
    "for key,frame in pdata.items():\n",
    "    frame['logE'] = EnergyMapping.Forward(frame['clusterE'].to_numpy())\n",
    "    frame['logECalib'] = EnergyMapping.Forward(frame['cluster_ENG_CALIB_TOT'].to_numpy())\n",
    "    frame['clusterEtaAbs'] = np.abs(frame['clusterEta'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_file = modelpath + 'scalers.save'\n",
    "scaler_branches = ['logE','logECalib','clusterEtaAbs']\n",
    "scalers = mu.setupScalers(pdata, scaler_branches, scaler_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a histogram of the regression vars, before scaling.\n",
    "fig, ax = plt.subplots(1,1,figsize=(7.5,5))\n",
    "labels = ['logE','logECalib']\n",
    "rvars = [np.concatenate([frame[x].to_numpy() for frame in pdata.values()]) for x in labels]\n",
    "\n",
    "pu.histogramOverlay(ax, rvars, labels, 'x', 'y',\n",
    "                    x_min = 0., x_max = 10., xbins = 100,\n",
    "                    normed = True, y_log = True,\n",
    "                    ps = plotstyle\n",
    "                   )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a histogram of the regression vars, after scaling.\n",
    "fig, ax = plt.subplots(1,1,figsize=(7.5,5))\n",
    "labels = ['s_logE','s_logECalib']\n",
    "rvars = [np.concatenate([frame[x].to_numpy() for frame in pdata.values()]) for x in labels]\n",
    "\n",
    "pu.histogramOverlay(ax, rvars, labels, 'x', 'y',\n",
    "                    x_min = -2.5, x_max = 2.5, xbins = 200,\n",
    "                    normed = True, y_log = True,\n",
    "                    ps = plotstyle\n",
    "                   )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may optionally perform some re-weighting of our training events. If using the `pion_reweighted` strategy, we will re-weight our single-pion training data to match the topo-cluster $p_T$ spectrum of our jet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider re-adding some sample weighting schemes\n",
    "sample_weights = {\n",
    "    key: np.full(np.sum(frame['train'].to_numpy()),1.)\n",
    "    for key,frame in pdata.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow/Keras Prep\n",
    "\n",
    "In this workflow we have the ability to train a number of models -- some will require additional data setup. Here, we have some basic setup they will all use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {} # keep track of the models -- note that we train multiple instances of each model\n",
    "histories = {} # keep track of histories of the models we train\n",
    "regressors = {} # keep track of the trained models (will be KerasRegressor objects)\n",
    "energy_names = {} # keep track of the names of regressed variables\n",
    "\n",
    "model_filename_suffixes = {\n",
    "    'pp':'_charged',\n",
    "    'p0':'_neutral'\n",
    "}\n",
    "\n",
    "energy_name_prefix = 'clusterE_pred_'\n",
    "\n",
    "#from keras.wrappers.scikit_learn import KerasRegressor # scikit_learn wrapper -- why do we use this, vs. native tf.keras approach like in classification notebook?\n",
    "#from tensorflow.keras.models import load_model\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "\n",
    "#from util.regression.models import baseline_nn_All_model, simple_dnn, resnet, resnet_wide\n",
    "from util.regression.models import baseline_nn_model, simple_dnn, resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some additional input preparation that we will need for some of our networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data.\n",
    "All_input = rdu.CombinedInput(pdata,\n",
    "                              pcells,\n",
    "                              branches = ['s_logE','s_clusterEtaAbs']\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \"all\" model\n",
    "\n",
    "Here we train a simple, fully-connected neural network that uses the calorimeter cells as input, along with reco energy and $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "model_key = 'all'\n",
    "energy_names[model_key] = energy_name_prefix + model_key\n",
    "lr = 1e-4\n",
    "decay = 0. # lr decay *within* each epoch 1e-6\n",
    "gamma = .1 # lr decay between epochs (via scheduler)\n",
    "min_delta = 0.0005\n",
    "patience = 5\n",
    "dropout = -1. # < 0 -> no dropout\n",
    "models[model_key] = baseline_nn_model(strategy, lr=lr, decay=decay, dropout=dropout)\n",
    "#print(models[model_key]().summary())\n",
    "\n",
    "# Set our training hyper-parameters.\n",
    "batch_size = 200 * ngpu\n",
    "epochs = 10\n",
    "verbose = 1\n",
    "regressors[model_key] = {}\n",
    "histories[model_key] = {}\n",
    "\n",
    "# Load/train the models, and evaluate them on all the data.\n",
    "for key in All_input.keys():\n",
    "    \n",
    "    tidx = pdata[key]['train']\n",
    "    vidx = pdata[key]['val'] \n",
    "    \n",
    "    model_dir = ''.join([modelpath, model_key])\n",
    "    model_filename = '{}/{}{}.h5'.format(model_dir,model_key,model_filename_suffixes[key])\n",
    "\n",
    "    regressors[model_key][key], histories[model_key][key] = rtu.TrainNetwork(\n",
    "        model=models[model_key],\n",
    "        modelfile = model_filename,\n",
    "        x_train = All_input[key][tidx],\n",
    "        y_train = pdata[key]['s_logECalib'][tidx],\n",
    "        x_valid = All_input[key][vidx],\n",
    "        y_valid = pdata[key]['s_logECalib'][vidx],\n",
    "        sample_weight=sample_weights[key],\n",
    "        callbacks = GetCallbacks(model_filename, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "        overwriteModel=overwriteModel,\n",
    "        finishTraining=finishTraining\n",
    "    )\n",
    "    \n",
    "    # Get predictions for all the data.\n",
    "    pdata[key][energy_names[model_key]] = EnergyMapping.Inverse(scalers[key]['logECalib'].inverse_transform(regressors[model_key][key].predict(All_input[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \"simple\" model\n",
    "\n",
    "Here we train another simple, fully-connected neural network that uses reco energy and $\\eta$ as input, along with depth information (vector of integrals of calorimeter images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data.\n",
    "All_input = rdu.DepthInput(pdata,\n",
    "                           pcells,\n",
    "                           branch_map = {\n",
    "                               's_logE':'energy',\n",
    "                               's_clusterEtaAbs':'eta'\n",
    "                           }\n",
    "                          )\n",
    "Split_input = rdu.DictionarySplit(All_input, pdata)\n",
    "train_input, valid_input = Split_input['train'], Split_input['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model\n",
    "model_key = 'simple'\n",
    "energy_names[model_key] = energy_name_prefix + model_key\n",
    "lr = 1e-4\n",
    "decay = 0. # lr decay *within* each epoch 1e-6\n",
    "gamma = .1 # lr decay between epochs (via scheduler)\n",
    "min_delta = 0.0005\n",
    "patience = 5\n",
    "dropout = -1. # < 0 -> no dropout\n",
    "models[model_key] = simple_dnn(strategy, lr=lr, decay=decay, dropout=dropout)\n",
    "#print(models[model_key]().summary())\n",
    "\n",
    "# Set our training hyper-parameters.\n",
    "batch_size = 200 * ngpu\n",
    "epochs = 10\n",
    "verbose = 1\n",
    "regressors[model_key] = {}\n",
    "histories[model_key] = {}\n",
    "\n",
    "# Load/train the models, and evaluate them on all the data.\n",
    "for key in All_input.keys():\n",
    "    \n",
    "    tidx = pdata[key]['train']\n",
    "    vidx = pdata[key]['val']  \n",
    "    \n",
    "    model_dir = ''.join([modelpath, model_key])\n",
    "    model_filename = '{}/{}{}.h5'.format(model_dir,model_key,model_filename_suffixes[key])\n",
    "\n",
    "    regressors[model_key][key], histories[model_key][key] = rtu.TrainNetwork(\n",
    "        model=models[model_key],\n",
    "        modelfile = model_filename,\n",
    "        x_train = train_input[key],\n",
    "        y_train = pdata[key]['s_logECalib'][tidx],\n",
    "        x_valid = valid_input[key],\n",
    "        y_valid = pdata[key]['s_logECalib'][vidx],\n",
    "        sample_weight=sample_weights[key],\n",
    "        callbacks = GetCallbacks(model_filename, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "        overwriteModel=overwriteModel,\n",
    "        finishTraining=finishTraining\n",
    "    )\n",
    "    \n",
    "    # Get predictions for all the data.\n",
    "    pdata[key][energy_names[model_key]] = EnergyMapping.Inverse(scalers[key]['logECalib'].inverse_transform(regressors[model_key][key].predict(All_input[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "We can also train an implementation of ResNet. More precisely, we use a ResNet model on the calorimeter images, and then mix in the energy and $\\eta$ at the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data.\n",
    "All_input = rdu.ResnetInput(pdata,\n",
    "                            pcells,\n",
    "                            branch_map = {\n",
    "                                's_logE':'energy',\n",
    "                                's_clusterEtaAbs':'eta'\n",
    "                            }\n",
    ")\n",
    "Split_input = rdu.DictionarySplit(All_input, pdata)\n",
    "train_input, valid_input = Split_input['train'], Split_input['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet\n",
    "model_key = 'resnet'\n",
    "energy_names[model_key] = energy_name_prefix + model_key\n",
    "lr = 1e-6\n",
    "decay = 0. # lr decay *within* each epoch 1e-6\n",
    "gamma = .1 # lr decay between epochs (via scheduler)\n",
    "min_delta = 0.0005\n",
    "patience = 5\n",
    "channels = 6\n",
    "filter_sets = [\n",
    "    [64,64,256],\n",
    "    [128,128,512]\n",
    "    #[256,256,1024],\n",
    "    #[512,512,2048]\n",
    "]         \n",
    "f_vals = [3,3] # [3,3,3,3] sizes of filters in middle of conv/identity blocks\n",
    "s_vals = [1,2] # [1,2,2,2] strides for each convolutional block\n",
    "i_vals = [2,3] # [2,3,5,2] number of identity blocks per stage\n",
    "input_shape = (128,16)\n",
    "augmentation = True # whether or not to augment data during training, by flipping in eta & phi\n",
    "\n",
    "models[model_key] = resnet(lr=lr, channels=channels, filter_sets=filter_sets, f_vals=f_vals, s_vals=s_vals, i_vals=i_vals, decay=decay, input_shape=input_shape, augmentation=augmentation)\n",
    "\n",
    "# Set our training hyper-parameters.\n",
    "batch_size = 200 * ngpu\n",
    "epochs = 5 # 200\n",
    "verbose = 1\n",
    "regressors[model_key] = {}\n",
    "histories[model_key] = {}\n",
    "\n",
    "# Load/train the models, and evaluate them on all the data.\n",
    "for key in All_input.keys():\n",
    "    \n",
    "    tidx = pdata[key]['train']\n",
    "    vidx = pdata[key]['val']  \n",
    "    \n",
    "    model_dir = ''.join([modelpath, model_key])\n",
    "    model_filename = '{}/{}{}.h5'.format(model_dir,model_key,model_filename_suffixes[key])\n",
    "\n",
    "    regressors[model_key][key], histories[model_key][key] = rtu.TrainNetwork(\n",
    "        model=models[model_key],\n",
    "        modelfile = model_filename,\n",
    "        x_train = train_input[key],\n",
    "        y_train = pdata[key]['s_logECalib'][tidx],\n",
    "        x_valid = valid_input[key],\n",
    "        y_valid = pdata[key]['s_logECalib'][vidx],\n",
    "        sample_weight=sample_weights[key],\n",
    "        callbacks = GetCallbacks(model_filename, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "        overwriteModel=overwriteModel,\n",
    "        finishTraining=finishTraining\n",
    "    )\n",
    "    \n",
    "    # Get predictions for all the data.\n",
    "    pdata[key][energy_names[model_key]] = EnergyMapping.Inverse(scalers[key]['logECalib'].inverse_transform(regressors[model_key][key].predict(All_input[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Mini\n",
    "\n",
    "In an effort to simplify our ResNet -- and get it to train faster -- we can consider just using a single channel for our images, or some subset of channels. We can re-use the input we prepared for our full ResNet, though we'll only need a portion of it. Note that if we only use a single channel, the rescaling (via `input_shape`) is redundant, and we should just set that to the original dimensions (and, in practice, remove the scaling entirely if we stick with just one channel).\n",
    "\n",
    "**TODO:** Reimplement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results (testing how well our network works)\n",
    "\n",
    "Now, let's plot some kinematics and network results. We'll make two groups of plots -- one for charged pions and one for neutral pions.\n",
    "\n",
    "Within each group of plots, we'll make two plots for each quantity -- one made using just the training data, and then one made using all the data (training + whatever we excluded -- but still excluding events with `cluster_ENG_CALIB_TOT` $< 0$ since these blow up network output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience below\n",
    "training_frames = {key:frame[training_indices[key]] for key,frame in data_frames.items()}\n",
    "validation_frames = {key:frame[validation_indices[key]] for key,frame in data_frames.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_key in energy_names.keys():\n",
    "    results = rpu.EnergySummary(\n",
    "        train_dfs=training_frames, \n",
    "        valid_dfs=validation_frames, \n",
    "        data_dfs=data_frames, \n",
    "        energy_name=energy_names[model_key], \n",
    "        model_name=model_key, \n",
    "        plotpath=plotpath, \n",
    "        extensions=['png'], \n",
    "        plot_size=750, \n",
    "        strat='pion', \n",
    "        full=False,\n",
    "        ps=plotstyle\n",
    "    )\n",
    "    for canv in results['canv'].values(): canv.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making graphs of the network models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "for model_key in regressors.keys():\n",
    "    for key,reg in regressors[model_key].items():\n",
    "        graph_name = '{}{}_{}.png'.format(plotpath,model_key,key)\n",
    "        plot_model(reg.model, graph_name, show_shapes=True, rankdir='LR', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Make plots of mse and mae for training and validation (if present)\n",
    "\n",
    "for model_key in histories.keys():\n",
    "    for dkey in histories[model_key].keys():\n",
    "    \n",
    "        dname = key_conversion[dkey]\n",
    "        \n",
    "        epochs = len(histories[model_key][dkey]['mae'])\n",
    "        epoch_ticks = epochs\n",
    "        if(epoch_ticks > 10): epoch_ticks = epoch_ticks/2\n",
    "        x = np.arange(epochs) + 1\n",
    "        fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "        keys = ['mae','val_mae']\n",
    "        lines = [histories[model_key][dkey][key] for key in keys]\n",
    "        pu.multiplot_common(ax[0], x,lines, keys, y_min=1.0e-3, y_max=10., y_log=True, x_ticks=epoch_ticks, xlabel = 'Epoch', ylabel = 'MAE', title='Mean Avg. Error for {} ({})'.format(model_key,dname), ps=plotstyle)\n",
    "    \n",
    "        keys = ['mse','val_mse']\n",
    "        lines = [histories[model_key][dkey][key] for key in keys]\n",
    "        pu.multiplot_common(ax[1], x,lines, keys, y_min=1.0e-3, y_max=10., y_log=True, x_ticks=epoch_ticks, xlabel = 'Epoch', ylabel = 'MSE', title='Mean Sq. Error for {} ({})'.format(model_key,dname), ps=plotstyle)\n",
    "    \n",
    "        # add grids\n",
    "        for axis in ax.flatten():\n",
    "            axis.grid(True,color=plotstyle.grid_plt)\n",
    "\n",
    "        qu.SaveSubplots(fig, ax, ['mae_{}_{}'.format(model_key,dkey), 'mse_{}_{}'.format(model_key,dkey)], savedir=plotpath)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
