{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopoCluster Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple, stripped-down notebook for training networks. I've removed most of the models that are present in `TopoClusterRegressionRewrite.ipynb`, as well as most of the plots (I find that the multitude of plots makes things a bit cumbersome and hard to navigate -- I'll see if I can change the way they are displayed later on).\n",
    "\n",
    "Here, we just train the so-called `all` model, which uses images from all $6$ calo layers. We train two versions, for charged and neutral pions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML fitting/loading/saving settings\n",
    "loadModel = False # if false, then run trainings directly. otherwise load the file.\n",
    "saveModel = True # if true, save the current model to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's choose our training data (and associated strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data choice\n",
    "# options are jet, pion, pion_reweighted\n",
    "strat = 'pion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries.\n",
    "import sys, os, uuid, glob, pickle\n",
    "import numpy as np\n",
    "import pandas as pd # we will use some uproot/pandas interplay here.\n",
    "import uproot as ur\n",
    "import ROOT as rt # used for plotting\n",
    "import joblib as jl # for saving scalers\n",
    "from numba import jit\n",
    "\n",
    "# Import our resolution utilities\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util       as pu\n",
    "from util import ml_util         as mu\n",
    "from util import qol_util        as qu\n",
    "from util import io_util         as iu\n",
    "\n",
    "rt.gStyle.SetOptStat(0)\n",
    "# use our custom dark style for plots\n",
    "plotstyle = qu.PlotStyle('dark')\n",
    "plotstyle.SetStyle() # still need to manually adjust legends, paves\n",
    "\n",
    "plotpath = path_prefix + 'regression/Plots/'\n",
    "modelpath = path_prefix + 'regression/Models/'\n",
    "paths = [plotpath, modelpath]\n",
    "for path in paths:\n",
    "    try: os.makedirs(plotpath)\n",
    "    except: pass\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "cell_shapes = {layers[i]:(len_eta[i],len_phi[i]) for i in range(len(layers))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Get the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me lay out some definitions, so it's clear as to what the data is.\n",
    "\n",
    "We have a number of different \"strategies\" (the `strat` variable at the top). These correspond to different choices of training, validation and testing datasets.\n",
    "\n",
    "1. `pion`: We train and validate the network using our pion gun data.\n",
    "\n",
    "2. `pion_reweighted`: This is the same as `pion`, except that our training data is reweighted using a jet dataset (via their reco topo-cluster $p_T$ distributions), that corresponds with QCD dijet events.\n",
    "\n",
    "3. `jet`: We train and validate the network using our jet data. This is a facsimile dataset -- we do not know the actual labels of the jet data topo-clusters, so we have assigned labels by matching clusters to truth-level pions in $(\\eta,\\phi)$.\n",
    "\n",
    "The validation performed for these networks is effectively being done on some \"holdout\" dataset from training -- it will by definition have similar kinematics, being drawn from the same set of events. The more interesting test -- how our energy regression performs in tandem with classification on our *unlabeled* jet dataset, will be handled in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now determine which files we get training and validation data from. Depends on our strategy.\n",
    "if(strat == 'pion' or strat == 'pion_reweighted'):\n",
    "    data_dir = path_prefix + 'data/pion/'\n",
    "    data_filenames = {'pp':data_dir+'piplus.root','pm':data_dir+'piminus.root','p0':data_dir+'pi0.root'}\n",
    "    \n",
    "elif(strat == 'jet'):\n",
    "    data_dir = path_prefix + 'jets/training/'\n",
    "    data_filenames = {'pp':data_dir+'piplus.root','p0':data_dir+'pi0.root'}\n",
    "\n",
    "# adjust our model and plot paths, so that they are unique for each strategy\n",
    "paths = [modelpath, plotpath]\n",
    "for i in range(len(paths)):\n",
    "    path = paths[i]\n",
    "    path = path + strat\n",
    "    try: os.makedirs(path)\n",
    "    except: pass\n",
    "    path = path + '/'\n",
    "    paths[i] = path\n",
    "modelpath, plotpath = paths\n",
    "\n",
    "# we get uproot trees and pandas DataFrames,\n",
    "# for training + validation\n",
    "tree_name = 'ClusterTree'\n",
    "branches = ['truthE', 'clusterE', 'clusterPt', 'clusterEta', 'cluster_ENG_CALIB_TOT']\n",
    "\n",
    "data_trees = {key:ur.open(val)[tree_name] for key,val in data_filenames.items()}\n",
    "data_frames = {key:val.pandas.df(branches,flatten=False) for key,val in data_trees.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing training, validation and testing samples\n",
    "\n",
    "Here, we're going to arrange our dataset into training, validation and testing samples (by defining the indices of these categories).\n",
    "\n",
    "\n",
    "As we're taking logarithms  of `clusterE` and `cluster_ENG_CALIB_TOT`, we will always perform cuts to have `clusterE` > 0 for all datasets, and `cluster_ENG_CALIB_TOT` > 0 for training. \n",
    "\n",
    "On top of those cuts, we're free to apply additional cuts to training, validation and testing data. We can do them below, as we pick event indices for each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we treat $\\pi^+$ and $\\pi^-$ as the same, let's combine them so that we have *charged* and *neutral* pions. We will store all the charged pions under the key `pp`, and delete the key `pm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_conversion = {'pp':'charged pion','p0':'neutral pion'}\n",
    "# combining dataframes\n",
    "if('pm' in data_frames.keys()):\n",
    "    data_frames['pp'] = data_frames['pp'].append(data_frames['pm'])\n",
    "    del data_frames['pm']\n",
    "    \n",
    "    data_trees['pp'] = [data_trees['pp'],data_trees['pm']]\n",
    "    data_trees['p0'] = [data_trees['p0']]\n",
    "    del data_trees['pm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frames = {}\n",
    "validation_frames = {}\n",
    "\n",
    "# First, the minimum energy cut that we will always apply to data. Anything that fails to pass this cut will be discarded,\n",
    "# we will never evaluate on events that don't pass this cut.\n",
    "global_energy_cut = 0.\n",
    "\n",
    "# We apply a lower cut on cluster_ENG_CALIB_TOT, as very low-energy clusters can throw off training.\n",
    "energy_cut = [0., -1.]\n",
    "if(strat == 'pion' or strat == 'pion_reweighted'): \n",
    "    energy_cut[0] = 0. # GeV (e.g. 5.0e-1 by default)\n",
    "    energy_cut[1] = -1.\n",
    "    \n",
    "elif(strat == 'jet'): \n",
    "    energy_cut[0] = 5.0e-2 # GeV\n",
    "    energy_cut[1] = -1.\n",
    "\n",
    "data_indices = {} # indices of all usable data, i.e. non-zero energy\n",
    "training_indices = {} # indices of events actually used for training\n",
    "validation_indices = {} # indices of events not used for training (but usable)\n",
    "\n",
    "# percent of events to hand over from training to testing\n",
    "testing_frac = 0.2\n",
    "rng = np.random.default_rng() # for shuffling indices when splitting training/testing\n",
    "for key in data_frames.keys():\n",
    "    n = len(data_frames[key])\n",
    "    eng_calib_tot = data_frames[key]['cluster_ENG_CALIB_TOT'].to_numpy()\n",
    "    selected_indices = eng_calib_tot > energy_cut[0]\n",
    "    if(energy_cut[1] > 0.): selected_indices = selected_indices * (eng_calib_tot < energy_cut[1])\n",
    "    \n",
    "    selected_indices = selected_indices.nonzero()[0] # from boolean array to a list of actual indices\n",
    "    rng.shuffle(selected_indices)\n",
    "    n_test = int(testing_frac * len(selected_indices))\n",
    "    \n",
    "    # making boolean arrays to select events -- arrays are of same length as dataframe\n",
    "    validation_indices[key] = np.full(n,False)\n",
    "    training_indices[key] = np.full(n,False)\n",
    "    validation_indices[key][np.sort(selected_indices[:n_test])] = True\n",
    "    training_indices[key][np.sort(selected_indices[n_test:])] = True\n",
    "\n",
    "    data_indices[key] = (data_frames[key]['cluster_ENG_CALIB_TOT'] > global_energy_cut).to_numpy()\n",
    "    data_frames[key] = data_frames[key][data_indices[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data_frames.keys():\n",
    "    print('Number of {type} training/testing events: {val1}/{val2}'.format(type=key_conversion[key], val1 = np.sum(training_indices[key]), val2 = np.sum(validation_indices[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have many more charged pions than neutral pions, so this *may* result in our charged pion regression being better-trained (unless the stats for both are sufficiently high)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining regression variables\n",
    "\n",
    "Beyond the information already present in the data, we may want to pre-compute some extra regression inputs.\n",
    "\n",
    "The difference between computing them here, on the whole dataset, versus doing it in batch as part of the network itself, is that we can also define some `scalers` based on these variables. This will allow us to scale them across the dataset, for example to get them into the interval of \\[0,1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1.\n",
    "b = 1.\n",
    "EnergyMapping = iu.LinLogMapping(b=b,m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a test of our mapping to demonstrate its numerical stability\n",
    "# TODO: Fix legend sizing, it is flaky (and needs re-running)\n",
    "x=np.linspace(-4.,4.,4000)\n",
    "b_vals=[0.,.1,1.,1.0e14]\n",
    "m_vals=[1.]\n",
    "iu.MapStabilityTest(iu.LinLogMapping, b_vals=b_vals, m_vals=m_vals, x=x, ps=plotstyle, savedir=plotpath, legend_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our `LinLogMapping` is pretty numerically stable as long as $b \\gg m$. I think this is the expected behaviour, given the instabilities with logarithms that we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some regression vars\n",
    "for key,frame in data_frames.items():\n",
    "    frame['logE'] = EnergyMapping.Forward(frame['clusterE'].to_numpy())\n",
    "    frame['logECalib'] = EnergyMapping.Forward(frame['cluster_ENG_CALIB_TOT'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load/create scalers\n",
    "if(loadModel):\n",
    "    # Fetch the scalers associated with the regression models\n",
    "    scaler_file = modelpath + 'scalers.save' # name is always the same (for now)\n",
    "    scalers = jl.load(scaler_file)\n",
    "    scaler_e = scalers['e']\n",
    "    scaler_cal = scalers['cal']\n",
    "    scaler_eta = scalers['eta']\n",
    "\n",
    "else:\n",
    "    scaler_e = {key:StandardScaler() for key in data_frames.keys()}\n",
    "    scaler_cal = {key:StandardScaler() for key in data_frames.keys()}\n",
    "    scaler_eta = {key:StandardScaler() for key in data_frames.keys()}\n",
    "\n",
    "    # fit our scalers, using the training data\n",
    "    for key, frame in data_frames.items():\n",
    "        scaler_e[key].fit(frame['logE'][training_indices[key]].to_numpy().reshape(-1,1))\n",
    "        scaler_cal[key].fit(frame['logECalib'][training_indices[key]].to_numpy().reshape(-1,1))\n",
    "        scaler_eta[key].fit(frame['clusterEta'][training_indices[key]].to_numpy().reshape(-1,1))\n",
    "\n",
    "# apply the scalers to all data\n",
    "for key, frame in data_frames.items():\n",
    "    frame['s_logE'] = scaler_e[key].transform(frame['logE'].to_numpy().reshape(-1,1))\n",
    "    frame['s_logECalib'] = scaler_cal[key].transform(frame['logECalib'].to_numpy().reshape(-1,1))\n",
    "    frame['s_eta'] = scaler_eta[key].transform(frame['clusterEta'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we save our scalers to a file, so that we can use them when evaluating the model elsewhere\n",
    "if(saveModel):\n",
    "    scalers = {'e':scaler_e, 'cal':scaler_cal, 'eta':scaler_eta}\n",
    "    scaler_file = modelpath + 'scalers.save'\n",
    "    if(saveModel and not loadModel): jl.dump(scalers,scaler_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish training data preparation, we concatenate the calorimeter images, and then combine them with columns `s_logE` and `s_Eta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a handy function for taking a DataFrame (with our scaler-derived columns) and our tree with calo images, and getting the actual network input we need. The `indices` argument is just used for `dtree`, whereas `dframe` should already have the indices applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombinedInput(dframe, dtree, indices=-1, input_keys = ['s_logE','s_eta'], layers=['EMB1','EMB2','EMB3','TileBar0','TileBar1','TileBar2']):\n",
    "    # Prepare the calo images for input to training.\n",
    "    l = len(layers) * len(dtree.keys())\n",
    "    i = 0\n",
    "    pfx = 'Loading calo images:      '\n",
    "    sfx = 'Complete'\n",
    "    bl = 50\n",
    "    qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "    calo_images = {}\n",
    "    for key in dtree.keys():\n",
    "        calo_images[key] = {}\n",
    "    \n",
    "        for layer in layers:\n",
    "            if(indices != -1): calo_images[key][layer] = mu.setupCells(dtree[key],layer, indices = indices[key])\n",
    "            else: calo_images[key][layer] = mu.setupCells(dtree[key],layer)\n",
    "            i += 1\n",
    "            qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "        \n",
    "    # Concatenate images, and prepare our combined input.\n",
    "    All_input = {}\n",
    "    keys = list(calo_images.keys())\n",
    "    l = 3 * len(keys)\n",
    "    i = 0\n",
    "    pfx = 'Preparing combined input: '\n",
    "    qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "    for key in keys:\n",
    "        combined_images = np.concatenate(tuple([calo_images[key][layer] for layer in layers]), axis=1)\n",
    "        del calo_images[key] # delete this element of calo_images, it has been copied and is no longer needed\n",
    "        i = i + 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "        s_combined,scaler_combined = mu.standardCells(combined_images, layers)\n",
    "        i = i + 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "        \n",
    "        All_input[key] = np.column_stack([dframe[key][x][indices[key]] for x in input_keys] + [s_combined])\n",
    "        #All_input[key] = np.column_stack((dframe[key]['s_logE'], dframe[key]['s_eta'],s_combined))\n",
    "        i = i + 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "    return All_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above function is a dictionary whose keys correspond with the different data categories (e.g. `p0` and `pp`, for neutral and charged pions). Each dictionary entry is of shape `(N,938)`. The first dimension is the number of clusters, and the second results from the two inputs for energy and eta, plus the concatenation of all calorimeter images (with $936$ pixels total)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may optionally perform some re-weighting of our training events. If using the `pion_reweighted` strategy, we will re-weight our single-pion training data to match the topo-cluster $p_T$ spectrum of our jet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = {}\n",
    "if(strat == 'pion_reweighted'):\n",
    "    pt_min = 0.\n",
    "    pt_max = 20.\n",
    "    pt_bins = 100\n",
    "        \n",
    "    # Get the jet files for reweighting. We look in jets/ to make sure we only use the relevant files.\n",
    "    jet_files = glob.glob(path_prefix + 'jets/data/pion/*.root')\n",
    "    \n",
    "    for key in training_frames.keys():\n",
    "        h_train    = rt.TH1F(qu.RN(), 'h_train',   pt_bins, pt_min, pt_max)\n",
    "        h_reweight = rt.TH1F(qu.RN(), 'h_reweight',pt_bins, pt_min, pt_max)\n",
    "\n",
    "        # fill training distribution\n",
    "        training_vals = data_frames[key]['clusterPt'][training_indices[key]].to_numpy()\n",
    "        for entry in training_vals: h_train.Fill(entry)\n",
    "        h_train.Scale(1./h_train.Integral())\n",
    "        \n",
    "        # fill the reweighting distribution\n",
    "        for file in jet_files:\n",
    "            for entry in ur.open(file)['ClusterTree'].array('clusterPt').flatten():\n",
    "                h_reweight.Fill(entry)\n",
    "        h_reweight.Scale(1./h_reweight.Integral())\n",
    "        h_reweight = h_reweight / h_train\n",
    "        \n",
    "        # now get a list of weights for our events\n",
    "        sample_weights[key] = np.array([h_reweight.GetBinContent(h_reweight.FindBin(x)) for x in training_vals])\n",
    "    \n",
    "#else:  sample_weights = {key:np.full(len(All_input[key]), 1.) for key in All_input.keys()}\n",
    "    \n",
    "else:  sample_weights = {key:np.full(int(np.sum(training_indices[key])), 1.) for key in training_indices.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Data Prep\n",
    "\n",
    "Our ResNet approach will require some additional data preparation (we may also use this for other networks in the future...). Its input data structure could possibly be derived from the output of `CombinedInput()`, but for the time being we'll just make a separate function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResnetInput(dframe, dtree, training_indices, validation_indices, data_indices, layers, cell_shapes):\n",
    "    \n",
    "    # prepare input for ResNet\n",
    "    l = len(layers) * len(dtree.keys())\n",
    "    i = 0\n",
    "    pfx = 'Loading calo images:      '\n",
    "    sfx = 'Complete'\n",
    "    bl = 50\n",
    "    qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "    rn_input = {}\n",
    "    for key in dtree.keys():\n",
    "        rn_input[key] = {}\n",
    "    \n",
    "        for layer in layers:\n",
    "            rn_input[key][layer] = mu.setupCells(dtree[key],layer, indices=data_indices[key])\n",
    "            i += 1\n",
    "            qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "    # Unflatten images. Note that the key names match those defined within resnet model in models.py, which are currently hard-coded.\n",
    "    for key,imageset in rn_input.items():\n",
    "        rn_input[key] = {'input' + str(i):imageset[layer].reshape(tuple([-1] + list(cell_shapes[layer]))) for i,layer in enumerate(layers)}\n",
    "        rn_input[key]['energy'] = dframe[key]['s_logE'].to_numpy()\n",
    "        rn_input[key]['eta'   ] = dframe[key]['s_eta' ].to_numpy()\n",
    "    return rn_input\n",
    "        \n",
    "# Splitting things into training and validation -- as we use a dictionary structure it's a bit more involved than for CombinedInput.\n",
    "def ResnetSplit(rn_input, training_indices, validation_indices):\n",
    "    # Now explicitly split things up into training and validation data.\n",
    "    rn_train = {\n",
    "        key:{\n",
    "            input_key:val[training_indices[key]] for input_key,val in dset.items()\n",
    "        }\n",
    "        for key,dset in rn_input.items()\n",
    "    }\n",
    "\n",
    "    rn_valid = {\n",
    "        key:{\n",
    "            input_key:val[validation_indices[key]] for input_key,val in dset.items()\n",
    "        }\n",
    "        for key,dset in rn_input.items()\n",
    "    }\n",
    "    return {'train':rn_train, 'valid':rn_valid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow/Keras Prep\n",
    "\n",
    "In this workflow we have the ability to train a number of models -- some will require additional data setup. Here, we have some basic setup they will all use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {} # keep track of the models -- note that we train multiple instances of each model\n",
    "histories = {} # keep track of histories of the models we train\n",
    "regressors = {} # keep track of the trained models (will be KerasRegressor objects)\n",
    "energy_names = {} # keep track of the names of regressed variables\n",
    "\n",
    "model_filename_suffixes = {\n",
    "    'pp':'_charged',\n",
    "    'p0':'_neutral'\n",
    "}\n",
    "\n",
    "energy_name_prefix = 'clusterE_pred_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor # scikit_learn wrapper -- why do we use this, vs. native tf.keras approach like in classification notebook?\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import baseline_nn_All_model, resnet\n",
    "import training_utils as tu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \"all\" model\n",
    "\n",
    "Here we train a simple, fully-connected neural network that uses the calorimeter cells as input, along with energy and $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "model_key = 'all'\n",
    "energy_names[model_key] = energy_name_prefix + model_key\n",
    "lr = 1e-4\n",
    "decay = 1e-6\n",
    "dropout = -1. # < 0 -> no dropout\n",
    "models[model_key] = baseline_nn_All_model(strategy, lr=lr, decay=decay, dropout=dropout)\n",
    "#print(models[model_key]().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our regressors (actual networks) that we will fit (train).\n",
    "batch_size = 200 * ngpu\n",
    "epochs = 2 # 20\n",
    "verbose = 1\n",
    "regressors[model_key] = {key: KerasRegressor(build_fn=models[model_key], batch_size=batch_size, epochs=epochs, verbose=verbose) for key in training_indices.keys()}\n",
    "histories[model_key] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data.\n",
    "All_input = CombinedInput(data_frames, data_trees, data_indices)\n",
    "\n",
    "# Load/train the models, and then evaluate them on all the data.\n",
    "for key in All_input.keys():\n",
    "    tidx = training_indices[key]\n",
    "    vidx = validation_indices[key]\n",
    "    history = tu.PrepNetwork(\n",
    "        regressor=regressors[model_key][key],\n",
    "        model_key=model_key,\n",
    "        suffix=model_filename_suffixes[key],\n",
    "        modelpath=modelpath,\n",
    "        loadModel=loadModel,\n",
    "        x_train = All_input[key][tidx],\n",
    "        y_train = data_frames[key]['s_logECalib'][tidx],\n",
    "        x_valid = All_input[key][vidx],\n",
    "        y_valid = data_frames[key]['s_logECalib'][vidx],\n",
    "        sample_weight=sample_weights[key],\n",
    "        saveModel=saveModel\n",
    "    )\n",
    "    histories[model_key][key] = history\n",
    "    # Get predictions for all the data.\n",
    "    data_frames[key][energy_names[model_key]] = EnergyMapping.Inverse(scaler_cal[key].inverse_transform(regressors[model_key][key].predict(All_input[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "We can also train an implementation of ResNet. More precisely, we use a ResNet model on the calorimeter images, and then mix in the energy and $\\eta$ at the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "model_key = 'resnet'\n",
    "energy_names[model_key] = energy_name_prefix + model_key\n",
    "lr = 5e-5\n",
    "channels = 6\n",
    "models[model_key] = resnet(strategy, lr=lr, channels=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our regressors (actual networks) that we will fit (train).\n",
    "batch_size = 20 * ngpu\n",
    "epochs = 2 # 20\n",
    "verbose = 1\n",
    "input_shape = (128,16)\n",
    "regressors[model_key] = {key: KerasRegressor(build_fn=models[model_key], input_shape=input_shape, batch_size=batch_size, epochs=epochs, verbose=verbose) for key in training_indices.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data.\n",
    "All_input = ResnetInput(data_frames, data_trees, data_indices, layers, cell_shapes)\n",
    "Split_input = ResnetSplit(All_input, training_indices, validation_indices)\n",
    "train_input = Split_input['train']\n",
    "valid_input = Split_input['valid']\n",
    "\n",
    "for key in train_input.keys():\n",
    "    tidx = training_indices[key]\n",
    "    vidx = validation_indices[key]\n",
    "    history = tu.PrepNetwork(\n",
    "        regressor=regressors[model_key][key],\n",
    "        model_key=model_key,\n",
    "        suffix=model_filename_suffixes[key],\n",
    "        modelpath=modelpath,\n",
    "        loadModel=loadModel,\n",
    "        x_train = train_input[key],\n",
    "        y_train = data_frames[key]['s_logECalib'][tidx],\n",
    "        x_valid = valid_input[key],\n",
    "        y_valid = data_frames[key]['s_logECalib'][vidx],\n",
    "        sample_weight=sample_weights[key],\n",
    "        saveModel=saveModel\n",
    "    )\n",
    "    histories[model_key][key] = history\n",
    "    # Get predictions for all the data.\n",
    "    data_frames[key][energy_names[model_key]] = EnergyMapping.Inverse(scaler_cal[key].inverse_transform(regressors[model_key][key].predict(All_input[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Mini\n",
    "\n",
    "In an effort to simplify our ResNet -- and get it to train faster -- we can consider just using a single channel for our images, corresponding to `EMB1`. We can re-use the input we prepared for our full ResNet, though we'll only need a portion of it and the `input_shape` will simply be the shape of the original input itself (making the scaling redundant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_keys = ['input0','energy','eta']\n",
    "\n",
    "mini_input = {\n",
    "    key:{mkey:val[mkey] for mkey in mini_keys}\n",
    "    for key,val in All_input.items()\n",
    "}\n",
    "\n",
    "mini_train = {\n",
    "    key:{mkey:val[mkey] for mkey in mini_keys}\n",
    "    for key,val in train_input.items()\n",
    "}\n",
    "\n",
    "mini_valid = {\n",
    "    key:{mkey:val[mkey] for mkey in mini_keys}\n",
    "    for key,val in valid_input.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "model_key = 'resnet_mini'\n",
    "energy_names[model_key] = energy_name_prefix + model_key\n",
    "lr = 5e-5\n",
    "channels = 1\n",
    "models[model_key] = resnet(strategy, lr=lr, channels=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our regressors (actual networks) that we will fit (train).\n",
    "batch_size = 100 * ngpu\n",
    "epochs = 4 # 20\n",
    "verbose = 1\n",
    "input_shape = (128,16)\n",
    "regressors[model_key] = {key: KerasRegressor(build_fn=models[model_key], input_shape=input_shape, batch_size=batch_size, epochs=epochs, verbose=verbose) for key in training_indices.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in train_input.keys():\n",
    "    tidx = training_indices[key]\n",
    "    vidx = validation_indices[key]\n",
    "    history = tu.PrepNetwork(\n",
    "        regressor=regressors[model_key][key],\n",
    "        model_key=model_key,\n",
    "        suffix=model_filename_suffixes[key],\n",
    "        modelpath=modelpath,\n",
    "        loadModel=loadModel,\n",
    "        x_train = mini_train[key],\n",
    "        y_train = data_frames[key]['s_logECalib'][tidx],\n",
    "        x_valid = mini_valid[key],\n",
    "        y_valid = data_frames[key]['s_logECalib'][vidx],\n",
    "        sample_weight=sample_weights[key],\n",
    "        saveModel=saveModel\n",
    "    )\n",
    "    histories[model_key][key] = history\n",
    "    # Get predictions for all the data.\n",
    "    data_frames[key][energy_names[model_key]] = EnergyMapping.Inverse(scaler_cal[key].inverse_transform(regressors[model_key][key].predict(mini_input[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the models and save them, or load them from files.\n",
    "# model_dir = modelpath + '/' + model_key\n",
    "# try: os.makedirs(model_dir)\n",
    "# except: pass\n",
    "\n",
    "# model_file_names = {key: '{}/{}{}.h5'.format(model_dir,model_key,suffix) for key,suffix in model_filename_suffixes.items()}\n",
    "# assert (set(model_file_names.keys()) == set(training_indices.keys()))\n",
    "\n",
    "# histories[model_key] = {}\n",
    "# for key, filename in model_file_names.items():\n",
    "#     if not loadModel:  \n",
    "#         histories[model_key][key] = regressors[model_key][key].fit(\n",
    "#             x=rn_mini_train[key],\n",
    "#             y=rn_label_train[key],\n",
    "#             validation_data=(\n",
    "#                 rn_mini_valid[key],\n",
    "#                 rn_label_valid[key]\n",
    "#             ),\n",
    "#             sample_weight = sample_weights[key]\n",
    "#         )\n",
    "#         histories[model_key][key] = histories[model_key][key].history\n",
    "\n",
    "#     else: \n",
    "#         regressors[model_key][key].model = load_model(filename)\n",
    "#         with open(filename.replace('.h5','.history'),'rb') as model_history_file:\n",
    "#             histories[model_key][key] = pickle.load(model_history_file)\n",
    "    \n",
    "\n",
    "#     if saveModel: \n",
    "#         regressors[model_key][key].model.save(filename)\n",
    "#         with open(filename.replace('.h5','.history'),'wb') as model_history_file:\n",
    "#             pickle.dump(histories[model_key][key], model_history_file)\n",
    "\n",
    "# # Get predictions for all data.\n",
    "# for key, frame in data_frames.items():\n",
    "#     frame[energy_names[model_key]] = EnergyMapping.Inverse(scaler_cal[key].inverse_transform(regressors[model_key][key].predict(rn_mini_input[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results (testing how well our network works)\n",
    "\n",
    "Now, let's plot some kinematics and network results. We'll make two groups of plots -- one for charged pions and one for neutral pions.\n",
    "\n",
    "Within each group of plots, we'll make two plots for each quantity -- one made using just the training data, and then one made using all the data (training + whatever we excluded -- but still excluding events with `cluster_ENG_CALIB_TOT` $< 0$ since these blow up network output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience below\n",
    "training_frames = {key:frame[training_indices[key]] for key,frame in data_frames.items()}\n",
    "validation_frames = {key:frame[validation_indices[key]] for key,frame in data_frames.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_utils import EnergySummary\n",
    "\n",
    "for model_key in energy_names.keys():\n",
    "    results = EnergySummary(\n",
    "        train_dfs=training_frames, \n",
    "        valid_dfs=validation_frames, \n",
    "        data_dfs=data_frames, \n",
    "        energy_name=energy_names[model_key], \n",
    "        model_name=model_key, \n",
    "        plotpath=plotpath, \n",
    "        extensions=['png'], \n",
    "        plot_size=750, \n",
    "        strat='pion', \n",
    "        full=False,\n",
    "        ps=plotstyle\n",
    "    )\n",
    "    for canv in results['canv'].values(): canv.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making graphs of the network models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "for model_key in regressors.keys():\n",
    "    for key,reg in regressors[model_key].items():\n",
    "        graph_name = '{}{}_{}.png'.format(plotpath,model_key,key)\n",
    "        plot_model(reg.model, graph_name, show_shapes=True, rankdir='LR', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Make plots of mse and mae for training and validation (if present)\n",
    "\n",
    "for model_key in histories.keys():\n",
    "    for dkey in histories[model_key].keys():\n",
    "    \n",
    "        dname = key_conversion[dkey]\n",
    "\n",
    "        x = np.arange(epochs) + 1\n",
    "        fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "        keys = ['mae','val_mae']\n",
    "        lines = [histories[model_key][dkey][key] for key in keys]\n",
    "        pu.multiplot_common(ax[0], x,lines, keys, y_min = 0.5, y_max = 1., xlabel = 'Epoch', ylabel = 'MAE', title='Mean Avg. Error for {} ({})'.format(model_key,dname), ps=plotstyle)\n",
    "    \n",
    "        keys = ['mse','val_mse']\n",
    "        lines = [histories[model_key][dkey][key] for key in keys]\n",
    "        pu.multiplot_common(ax[1], x,lines, keys, y_max = .7, xlabel = 'Epoch', ylabel = 'MSE', title='Mean Sq. Error for {} ({})'.format(model_key,dname), ps=plotstyle)\n",
    "    \n",
    "        # add grids\n",
    "        for axis in ax.flatten():\n",
    "            axis.grid(True,color=plotstyle.grid_plt)\n",
    "\n",
    "        qu.SaveSubplots(fig, ax, ['mae_{}_{}'.format(model_key,dkey), 'mse_{}_{}'.format(model_key,dkey)], savedir=plotpath)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
