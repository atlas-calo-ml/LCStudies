{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification of ATLAS Calorimeter Topo-Clusters (Jan)\n",
    "\n",
    "## This is a stripped-down version of Max's re-write, so I have removed *some* functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If true, force re-training even if a model already exists. Existing model will be lost!\n",
    "overwriteModel = False\n",
    "\n",
    "# If true, continue training and try to train to the last specified epoch.\n",
    "# If EarlyStopping was used, this may result in trying to further train a \"finished\" network.\n",
    "finishTraining = False\n",
    "\n",
    "# If no file extension, uses native TensorFlow format (.tf).\n",
    "# If 'h5', uses HDF5. HDF5 does not work for custom layers/classes! (unless you design them a certain way)\n",
    "file_extension = '.h5'\n",
    "if(file_extension != '' and '.' not in file_extension):\n",
    "    file_extension = '.' + file_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Navigation:\n",
    "- [Simple feed-forward Neural Network](#Simple-feed-forward-Neural-Network.)\n",
    "- [ROC Curve Scans](#ROC-Curve-Scans)\n",
    "- [Combination Network](#Combination-Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and some constants\n",
    "import os, sys, json, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LogNorm, TwoSlopeNorm\n",
    "import pandas as pd\n",
    "import ROOT as rt # I will use this for some plotting\n",
    "import uproot as ur\n",
    "\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "plotpath = path_prefix+'classifier/Plots/'\n",
    "modelpath = path_prefix+'classifier/Models/'\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu\n",
    "from util import qol_util as qu\n",
    "\n",
    "# Custom tensorflow.keras callbacks\n",
    "from util.keras.callbacks import GetCallbacks\n",
    "\n",
    "# Classification-specific utils\n",
    "from util.classification import training_util as ctu\n",
    "from util.classification import plot_util as cpu\n",
    "from util.classification import data_util as cdu\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "cell_shapes = {layers[i]:(len_eta[i],len_phi[i]) for i in range(len(layers))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy display names for each pion type\n",
    "pi_latex = {\n",
    "    'pi0': '\\(\\pi^{0}\\)',\n",
    "    'piplus': '\\(\\pi^{+}\\)',\n",
    "    'piminus': '\\(\\pi^{-}\\)',\n",
    "}\n",
    "\n",
    "pi_latex_mpl = {\n",
    "    'pi0': '$\\pi^{0}$',\n",
    "    'piplus': '$\\pi^{+}$'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting settings\n",
    "\n",
    "# plotting mode\n",
    "mode = 'dark'\n",
    "\n",
    "# xkcd -- turn this on for fun-looking (but marginally less useful) plots\n",
    "use_xkcd = False\n",
    "if(use_xkcd):\n",
    "    mode = 'light'\n",
    "    plt.xkcd(scale=.75,length=100,randomness=1)\n",
    "    \n",
    "# plotting style -- manages our color palette and object colors\n",
    "plotstyle = qu.PlotStyle(mode)\n",
    "    \n",
    "# some matplotlib-specific stuff\n",
    "params = {'legend.fontsize': 13,\n",
    "          'axes.labelsize': 18}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import our data from the `ROOT` files into a `pandas` DataFrame. The first cell takes care of scalars, and the second takes care of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pi+- vs. pi0 images\n",
    "source = 'pion' # also try 'jet'\n",
    "\n",
    "if(source == 'pion'):\n",
    "    inputpath = path_prefix+'data/pion/'\n",
    "    rootfiles = [\"pi0\", \"piplus\", \"piminus\"]\n",
    "    branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_sumCellE', 'cluster_ENG_CALIB_TOT', 'cluster_ENG_CALIB_OUT_T', 'cluster_ENG_CALIB_DEAD_TOT', 'cluster_EM_PROBABILITY', 'cluster_HAD_WEIGHT', 'cluster_OOC_WEIGHT', 'cluster_DM_WEIGHT', 'cluster_CENTER_MAG', 'cluster_FIRST_ENG_DENS', 'cluster_cell_dR_min', 'cluster_cell_dR_max', 'cluster_cell_dEta_min', 'cluster_cell_dEta_max', 'cluster_cell_dPhi_min', 'cluster_cell_dPhi_max', 'cluster_cell_centerCellEta', 'cluster_cell_centerCellPhi', 'cluster_cell_centerCellLayer', 'cluster_cellE_norm']\n",
    "elif(source == 'jet'):\n",
    "    inputpath = path_prefix+'jets/training/'\n",
    "    rootfiles = [\"pi0\", \"piplus\"]\n",
    "    branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_ENG_CALIB_TOT']\n",
    "else:\n",
    "    assert(False)\n",
    "\n",
    "trees = {\n",
    "    rfile : ur.open(inputpath+rfile+\".root\")['ClusterTree']\n",
    "    for rfile in rootfiles\n",
    "}\n",
    "pdata = {\n",
    "    ifile : itree.pandas.df(branches, flatten=False)\n",
    "    for ifile, itree in trees.items()\n",
    "}\n",
    "\n",
    "total = 0\n",
    "for key in rootfiles:\n",
    "    total += len(pdata[key])\n",
    "\n",
    "for key in rootfiles:\n",
    "    n = len(pdata[key])\n",
    "    print(\"Number of {a:<7} events: {b:>10}\\t({c:.1f}%)\".format(a=key, b = n, c = 100. * n / total))\n",
    "print(\"Total: {}\".format(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of events for each category may be quite different -- ultimately we want to train our classifier on a \"balanced\" dataset, where we have equal numbers of entries from each category.\n",
    "\n",
    "We're training our network to classify between $\\pi^\\pm$ and $\\pi^0$ events. Thus, we should ultimately merge our $\\pi^+$ and $\\pi^-$ data.\n",
    "\n",
    "Thus, we will first generate selected indices for all categories, such that the total number of events from each category is equal, and *then* we will merge things.\n",
    "\n",
    "Note that as we're dealing with DataFrames (`pdata`) and uproot trees (`trees`, whose contents get loaded into `pcells`), we have to be careful that when we merge data, we do it the same way for both sets of objects. Otherwise we might scramble our $\\pi^\\pm$ data -- which will matter *if* we ever want to use inputs beyond just the images (from `trees`) as network input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_indices = {}\n",
    "n_max = int(np.min(np.array([len(pdata[key]) for key in trees.keys()])))\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# If we have a piminus key, assume the dataset are piplus, piminus, pi0\n",
    "if('piminus' in trees.keys()):\n",
    "    n_indices['piplus']  = int(np.ceil((n_max / 2)))\n",
    "    n_indices['piminus'] = int(np.floor((n_max / 2)))\n",
    "    n_indices['pi0']     = n_max\n",
    "    \n",
    "# Otherwise, assume we already have piplus (or piplus + piminus) and pi0, no merging needed\n",
    "else: n_indices = {key:n_max for key in trees.keys}\n",
    "indices = {key:rng.choice(len(pdata[key]), n_indices[key], replace=False) for key in trees.keys()}\n",
    "\n",
    "# Make a boolean array version of our indices, since pandas is weird and doesn't handle non-bool indices?\n",
    "bool_indices = {}\n",
    "for key in pdata.keys():\n",
    "    bool_indices[key] = np.full(len(pdata[key]), False)\n",
    "    bool_indices[key][indices[key]] = True\n",
    "\n",
    "# Apply the (bool) indices to pdata\n",
    "for key in trees.keys():\n",
    "    pdata[key] = pdata[key][bool_indices[key]]\n",
    "\n",
    "# prepare pcells -- immediately apply our selected indices\n",
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer, indices = indices[ifile])\n",
    "        for layer in layers\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with the data extracted from the trees into pcells, we merge pdata and pcells as needed.\n",
    "# Note the order in which we concatenate things: piplus -> piplus + piminus.\n",
    "if('piminus' in trees.keys()):\n",
    "    print('Merging piplus and piminus.')\n",
    "    \n",
    "    # merge pdata\n",
    "    pdata['piplus'] = pdata['piplus'].append(pdata['piminus'])\n",
    "    del pdata['piminus']\n",
    "    \n",
    "    # merge contents of pcells\n",
    "    for layer in layers:\n",
    "        pcells['piplus'][layer] = np.row_stack((pcells['piplus'][layer],pcells['piminus'][layer]))\n",
    "    del pcells['piminus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata_merged, pcells_merged, plabels = cdu.DataPrep(pdata, pcells, layers, trainfrac=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the network on $\\pi^+$ and $\\pi^0$ events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few example images.\n",
    "\n",
    "These are the images that we will use to train our network (together with a few other variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# specify which cluster to plot\n",
    "cluster = 100\n",
    "\n",
    "# make the plot\n",
    "plt.cla(); plt.clf()\n",
    "\n",
    "fig, ax = plt.subplots(2,6,figsize=(60,20))\n",
    "fig.patch.set_facecolor(plotstyle.canv_plt)\n",
    "\n",
    "i = 0\n",
    "for ptype, pcell in pcells.items():\n",
    "    for layer in layers:\n",
    "        axis = ax.flatten()[i]\n",
    "        \n",
    "        image = pcell[layer][cluster].reshape(cell_shapes[layer])\n",
    "        vmin, vmax = np.min(image), np.max(image)\n",
    "        vmax = np.maximum(np.abs(vmin),np.abs(vmax))\n",
    "        vmin = -vmax\n",
    "        \n",
    "        if(vmax == 0. and vmin == 0.):\n",
    "            vmax = 0.1\n",
    "            vmin = -vmax\n",
    "        \n",
    "        norm = TwoSlopeNorm(vmin=vmin,vcenter=0.,vmax=vmax)\n",
    "        cmap = plt.get_cmap('BrBG')\n",
    "        \n",
    "        im = axis.imshow(\n",
    "            pcell[layer][cluster].reshape(cell_shapes[layer]), \n",
    "            extent=[-0.2, 0.2, -0.2, 0.2],\n",
    "            cmap=cmap, \n",
    "            origin='lower', \n",
    "            interpolation='nearest',\n",
    "            norm=norm\n",
    "        )\n",
    "                \n",
    "        #axis.colorbar()\n",
    "        axis.set_title('{a} in {b}'.format(a=pi_latex_mpl[ptype],b=layer))\n",
    "        axis.set_xlabel(\"$\\Delta\\phi$\")\n",
    "        axis.set_ylabel(\"$\\Delta\\eta$\")\n",
    "        \n",
    "        plotstyle.SetStylePlt(axis)\n",
    "        divider = make_axes_locatable(axis)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "        cb = fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "        plt.setp(plt.getp(cb.ax.axes, 'yticklabels'), color=plotstyle.text_plt)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "# show the plots\n",
    "plt.savefig(plotpath+'plots_pi0_plus_minus.png',transparent=True,facecolor=plotstyle.canv_plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few histograms.\n",
    "\n",
    "These are a bit uglier than the `matplotlib` ones Max made, but it's perhaps even easier to see any differences between $\\pi^\\pm$ and $\\pi^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.gStyle.SetOptStat(0)\n",
    "\n",
    "plotstyle.SetStyle()\n",
    "\n",
    "# For storing histograms and legends, to prevent overwriting. (TODO: Probably better ways to do this in PyROOT)\n",
    "histos = []\n",
    "legends = []\n",
    "\n",
    "qtys = ['cluster_nCells', 'clusterE', 'clusterEta', 'clusterPhi', 'cluster_EM_PROBABILITY', 'cluster_sumCellE']\n",
    "qty_labels = ['Cells/Cluster', 'Cluster Energy [GeV]', 'Cluster #eta', 'Cluster #phi', 'Cluster EMProb', 'Cluster SumCellE']\n",
    "qty_ranges = [(0,500), (50,200), (-0.8,0.8), (-4.,4.), (0.,1.), (0.,2500.)]\n",
    "\n",
    "if(source == 'jet'):\n",
    "    qtys = ['cluster_nCells', 'clusterE', 'clusterEta', 'clusterPhi']\n",
    "    qty_labels = ['Cells/Cluster', 'Cluster Energy [GeV]', 'Cluster #eta', 'Cluster #phi']\n",
    "    qty_ranges = [(0,300), (0,100), (-0.8,0.8), (-4.,4.)]\n",
    "\n",
    "# Set up a canvas.\n",
    "plot_size = 500\n",
    "nx = int(np.ceil(len(qtys) / 2))\n",
    "ny = 2\n",
    "n_pad = nx * ny\n",
    "canvas = rt.TCanvas('cluster_hists','c1',plot_size * nx,plot_size * ny)\n",
    "canvas.Divide(nx,ny)\n",
    "\n",
    "colors = {'piplus':rt.kRed,'pi0':rt.kBlue}\n",
    "styles = {'piplus':3440, 'pi0':3404}\n",
    "\n",
    "n_bins=20\n",
    "for i, (qty, label, rng) in enumerate(zip(qtys, qty_labels, qty_ranges)):\n",
    "    canvas.cd(i+1)\n",
    "    leg = rt.TLegend(0.7,0.8,0.9,0.9)\n",
    "    for ptype, p in pdata.items():\n",
    "        hist = rt.TH1F('h_'+str(ptype)+'_'+str(qty),'',n_bins,rng[0],rng[1])\n",
    "        for entry in p[qty]: hist.Fill(entry)\n",
    "        integral = hist.Integral()\n",
    "        if(integral != 0): hist.Scale(1./hist.Integral())\n",
    "        hist.SetLineColor(colors[ptype])\n",
    "        hist.SetLineWidth(2)\n",
    "        hist.SetFillColorAlpha(colors[ptype],0.5)\n",
    "        hist.SetFillStyle(styles[ptype])\n",
    "        hist.Draw('HIST SAME')\n",
    "        hist.GetXaxis().SetTitle(label)\n",
    "        hist.GetYaxis().SetTitle('Normalised events')\n",
    "        hist.SetMaximum(1.5 * hist.GetMaximum())\n",
    "        leg.AddEntry(hist,pi_latex[ptype],'f')\n",
    "        leg.Draw()\n",
    "        histos.append(hist)\n",
    "        legends.append(leg)\n",
    "    if(qty in ['cluster_nCells','clusterE', 'cluster_EM_PROBABILITY', 'cluster_sumCellE']): rt.gPad.SetLogy()\n",
    "canvas.Draw()\n",
    "canvas.SaveAs(plotpath+'hist_pi0_plus_minus.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow/Keras prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "#gpu_list = [\"/gpu:0\"] #[\"/gpu:0\",\"/gpu:1\",\"/gpu:2\",\"/gpu:3\"]\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "from util.classification.models import baseline_nn_model, simple_combine_model, resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare some callbacks (originally from our regression notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storing models and some of their metrics (acc, loss)\n",
    "models = {}\n",
    "model_history = {}\n",
    "model_scores = {}\n",
    "model_performance = {}\n",
    "\n",
    "# For storing info on ROC curves\n",
    "roc_fpr = {}\n",
    "roc_tpr = {}\n",
    "roc_thresh = {}\n",
    "roc_auc = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's add some info to our dictionaries that corresponds to the existing `EM LC Prob` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = 'LC EMProb'\n",
    "model_scores[model_key] = 1-pdata_merged[\"cluster_EM_PROBABILITY\"]\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    drawPlots=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feed-forward Neural Network.\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to train simple, feed-foward neural networks -- one per calo layer. These will be our \"baseline networks\".\n",
    "\n",
    "Note that while for most of the notebook, we'll train one instance of a network per model, whereas here we will explicitly train multiple instances as we're doing one instance *per calo layer*, each for the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5\n",
    "gamma = 0.1\n",
    "min_delta = 0.0001\n",
    "patience = 3\n",
    "dropout = 0.1 # < 0 -> no dropout\n",
    "#model = baseline_nn_model(strategy, lr=lr, dropout=dropout)\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "\n",
    "model_name = 'flat'\n",
    "model_dir = modelpath + model_name # directory for loading/saving flat models\n",
    "\n",
    "for layer in layers:\n",
    "    \n",
    "    model_key = '{}_{}'.format(model_name, layer)\n",
    "    modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "    npix = cell_shapes[layer][0] * cell_shapes[layer][1]\n",
    "    models[model_key] = baseline_nn_model(strategy, npix, lr=lr, dropout=dropout)\n",
    "    \n",
    "    # train the network\n",
    "    models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "        model=models[model_key], \n",
    "        modelfile=modelfile, \n",
    "        x_train = pcells_merged[layer][pdata_merged.train], \n",
    "        y_train = plabels[pdata_merged.train], \n",
    "        x_valid = pcells_merged[layer][pdata_merged.val], \n",
    "        y_valid = plabels[pdata_merged.val], \n",
    "        callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "        epochs=nepochs, \n",
    "        batch_size=batch_size, \n",
    "        verbose=verbose, \n",
    "        overwriteModel=overwriteModel,\n",
    "        finishTraining=finishTraining\n",
    "    )\n",
    "        \n",
    "    # get performance metric from test set\n",
    "    model_performance[model_key] = models[model_key].evaluate(\n",
    "        pcells_merged[layer][pdata_merged.test],\n",
    "        plabels[pdata_merged.test],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print('Finished layer {}.'.format(layer))\n",
    "    \n",
    "    # get/recalculate network scores for the dataset\n",
    "    model_scores[model_key] = models[model_key].predict(pcells_merged[layer])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at accuracy and loss, as well as ROC curves, for each network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotstyle,\n",
    "    plotpath=plotpath\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_flat',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Network\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a simple combination network. Its inputs will be the *outputs* of our simple, feed-forward neural networks from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'simple'\n",
    "model_dir = modelpath + model_name # directory for loading/saving simple combination network\n",
    "model_key = 'simpleCombine'\n",
    "\n",
    "model_scores_stack = np.column_stack( [model_scores['flat_{}'.format(layer)] for layer in layers])\n",
    "n_input = model_scores_stack.shape[1]\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.1\n",
    "min_delta = 0.0005\n",
    "patience = 5\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key,file_extension)\n",
    "models[model_key] = simple_combine_model(strategy, lr=lr, n_input=n_input)\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = model_scores_stack[pdata_merged.train],\n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = model_scores_stack[pdata_merged.val], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    model_scores_stack[pdata_merged.test],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(model_scores_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotstyle,\n",
    "    plotpath=plotpath,\n",
    "    model_keys = ['simpleCombine']\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_simple',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'flat_EMB1', 'simpleCombine']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor data prep -- key names match those defined within resnet model in models.py!\n",
    "pcells_merged_unflattened = {'input' + str(i):pcells_merged[key].reshape(tuple([-1] + list(cell_shapes[key]))) for i,key in enumerate(pcells_merged.keys())}\n",
    "\n",
    "rn_train = {key:val[pdata_merged.train] for key,val in pcells_merged_unflattened.items()}\n",
    "rn_valid = {key:val[pdata_merged.val] for key,val in pcells_merged_unflattened.items()}\n",
    "rn_test = {key:val[pdata_merged.test] for key,val in pcells_merged_unflattened.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train an instance of ResNet.\n",
    "\n",
    "To do this, we will want to appropriately up/downscale all of our calorimeter images, so they are all of the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in cell_shapes.items():\n",
    "    print(key,val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, it seems like a reasonable choice would be rescaling all images to be of shape `(128,16)`. That corresponds with the maximum dimensions along each axis, so we'll just need to do some upscaling. The nice thing of avoiding downscaling is that we are not losing information.\n",
    "\n",
    "We can run a quick test to make sure that our scaling is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test=True\n",
    "\n",
    "from util.keras.layers import ImageScaleBlock\n",
    "def TestImage(idxs):\n",
    "    images = [np.stack([rn_train['input{}'.format(x)][idx,:,:] for idx in idxs],axis=0) for x in range(6)]    \n",
    "    # need to add the last dimension, corresponding with channel (will be of size 1)\n",
    "    images = [np.expand_dims(im, axis=-1) for im in images]\n",
    "    return images\n",
    "\n",
    "if(image_test):\n",
    "    test_idx = np.arange(5)\n",
    "    image = TestImage(test_idx)\n",
    "    scaled_image = ImageScaleBlock((128,16),normalization=True)(image).numpy()\n",
    "    for i,im in enumerate(image):\n",
    "        integrals_old = np.sum(im,axis=(1,2)).flatten()\n",
    "        integrals_new = np.sum(scaled_image[:,:,:,i], axis=(1,2)).flatten()\n",
    "\n",
    "        ratio = integrals_old.copy()\n",
    "        ratio[ratio==0.] = 1.\n",
    "        ratio = integrals_new/ratio\n",
    "\n",
    "        integrals_old = '\\t\\t'.join(['{:.1e}'.format(x) for x in integrals_old])\n",
    "        integrals_new = '\\t\\t'.join(['{:.1e}'.format(x) for x in integrals_new])\n",
    "        ratio         = '\\t\\t'.join(['{:.1e}'.format(x) for x in ratio        ])\n",
    "\n",
    "        print('Integral {} = {}'.format(i,integrals_old))\n",
    "        print('           = {}'.format(integrals_new))\n",
    "        print('Ratios     = {}'.format(ratio))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_image_data_format('channels_last')\n",
    "model_name = 'resnet'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'resnet'\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 20 # large patience since loss sometimes fluctuates upwards for a bit?\n",
    "input_shape = (128,16)\n",
    "channels = 6\n",
    "augmentation=True\n",
    "normalization=True\n",
    "filter_sets = [\n",
    "    [64,64,256],\n",
    "    [128,128,512]\n",
    "    #[256,256,1024],\n",
    "    #[512,512,2048]\n",
    "]         \n",
    "f_vals = [3,3] # [3,3,3,3] sizes of filters in middle of conv/identity blocks\n",
    "s_vals = [1,2] # [1,2,2,2] strides for each convolutional block\n",
    "i_vals = [2,3] # [2,3,5,2] number of identity blocks per stage\n",
    "\n",
    "nepochs = 200\n",
    "batch_size = 50 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = resnet(\n",
    "    filter_sets=filter_sets, \n",
    "    lr=lr, \n",
    "    channels=channels, \n",
    "    f_vals=f_vals, \n",
    "    s_vals=s_vals, \n",
    "    i_vals=i_vals, \n",
    "    input_shape=input_shape, \n",
    "    augmentation=augmentation,\n",
    "    normalization=normalization\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_train, \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_valid, \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_test,\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotstyle,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=[model_key]\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_resnet',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'flat_EMB1', model_key]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Network: Take 2\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, we can also train a combination network where we use our ResNet as an additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_stack = np.column_stack((model_scores_stack, model_scores['resnet']))\n",
    "n_input = model_scores_stack.shape[1]\n",
    "\n",
    "nepochs = 50\n",
    "batch_size = 200*ngpu\n",
    "verbose = 2\n",
    "\n",
    "\n",
    "model_name = 'complex'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'complexCombine'\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.1\n",
    "min_delta = 0.001\n",
    "patience = 3\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = simple_combine_model(strategy, lr=lr, n_input=n_input)\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = model_scores_stack[pdata_merged.train],\n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = model_scores_stack[pdata_merged.val], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteMode,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    model_scores_stack[pdata_merged.test],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(model_scores_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotstyle,\n",
    "    plotpath=plotpath,\n",
    "    model_keys = ['complexCombine']\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_complex',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'simpleCombine', 'complexCombine']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
