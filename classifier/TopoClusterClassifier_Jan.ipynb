{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification of ATLAS Calorimeter Topo-Clusters (Jan)\n",
    "\n",
    "This is a stripped-down version of Max's re-write, so I have removed *some* functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If true, force re-training even if a model already exists. Existing model will be lost!\n",
    "overwriteModel = False\n",
    "\n",
    "# If true, continue training and try to train to the last specified epoch.\n",
    "# If EarlyStopping was used, this may result in trying to further train a \"finished\" network.\n",
    "finishTraining = False\n",
    "\n",
    "# If no file extension, uses native TensorFlow format (.tf).\n",
    "# If 'h5', uses HDF5. HDF5 does not work for custom layers/classes! (unless you design them a certain way)\n",
    "file_extension = '.h5'\n",
    "if(file_extension != '' and '.' not in file_extension):\n",
    "    file_extension = '.' + file_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Navigation:\n",
    "- [Simple feed-forward Neural Network](#Simple-feed-forward-Neural-Network)\n",
    "- [Combination Network](#Combination-Network)\n",
    "- [Convolutional Neural Networks](#Convolutional-Neural-Networks)\n",
    "    - [Single-calo-layer CNN](#Single-calo-layer-CNN's)\n",
    "    -[Combination Network (CNN's)](#Combination-Network:-Take-2)\n",
    "    - [Complex CNN's](#Convolutional-Neural-Networks:-More-complicated-architectures)\n",
    "        - [3 EMB layers, separate](#3-EMB-layers,-separate)\n",
    "        - [All layers, separate](#All-layers,-separate)\n",
    "        - [All layers, merged](#All-layers,-merged)\n",
    "        - [EMB merged + TileBar merged](#EMB-merged-+-TileBar-merged)\n",
    "        - [EMB merged + TileBar depth](#EMB-merged-+-TileBar-depth)\n",
    "- [ResNet](#ResNet)\n",
    "- [Combination Network (ResNet + simple NN)](#Combination-Network:-Take-3)\n",
    "\n",
    "- [Summary Plots](#Summary-Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "#import libraries and some constants\n",
    "import os, sys, json, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ROOT as rt # I will use this for some plotting\n",
    "import uproot as ur\n",
    "\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "plotpath = path_prefix+'classifier/Plots/'\n",
    "modelpath = path_prefix+'classifier/Models/'\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu\n",
    "from util import qol_util as qu\n",
    "\n",
    "# Custom tensorflow.keras callbacks\n",
    "from util.keras.callbacks import GetCallbacks\n",
    "\n",
    "# Classification-specific utils\n",
    "from util.classification import training_util as ctu\n",
    "from util.classification import plot_util as cpu\n",
    "from util.classification import data_util as cdu\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "cell_shapes = {layers[i]:(len_eta[i],len_phi[i]) for i in range(len(layers))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy display names for each pion type\n",
    "pi_latex = {\n",
    "    'pi0': '\\(\\pi^{0}\\)',\n",
    "    'piplus': '\\(\\pi^{+}\\)',\n",
    "    'piminus': '\\(\\pi^{-}\\)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting settings\n",
    "\n",
    "# plotting mode\n",
    "mode = 'dark'\n",
    "\n",
    "# xkcd -- turn this on for fun-looking (but marginally less useful) plots\n",
    "use_xkcd = False\n",
    "if(use_xkcd):\n",
    "    mode = 'light'\n",
    "    plt.xkcd(scale=.75,length=100,randomness=1)\n",
    "    \n",
    "# plotting style -- manages our color palette and object colors\n",
    "plotstyle = qu.PlotStyle(mode)\n",
    "    \n",
    "# some matplotlib-specific stuff\n",
    "params = {'legend.fontsize': 13,\n",
    "          'axes.labelsize': 18}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import our data from the `ROOT` files into a `pandas` DataFrame. The first cell takes care of scalars, and the second takes care of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pi0     events:     264167\t(23.2%)\n",
      "Number of piplus  events:     436949\t(38.4%)\n",
      "Number of piminus events:     436802\t(38.4%)\n",
      "Total: 1137918\n"
     ]
    }
   ],
   "source": [
    "# import pi+- vs. pi0 images\n",
    "source = 'pion' # also try 'jet'\n",
    "\n",
    "if(source == 'pion'):\n",
    "    inputpath = path_prefix+'data/pion/'\n",
    "    rootfiles = [\"pi0\", \"piplus\", \"piminus\"]\n",
    "    branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_sumCellE', 'cluster_ENG_CALIB_TOT', 'cluster_ENG_CALIB_OUT_T', 'cluster_ENG_CALIB_DEAD_TOT', 'cluster_EM_PROBABILITY', 'cluster_HAD_WEIGHT', 'cluster_OOC_WEIGHT', 'cluster_DM_WEIGHT', 'cluster_CENTER_MAG', 'cluster_FIRST_ENG_DENS', 'cluster_cell_dR_min', 'cluster_cell_dR_max', 'cluster_cell_dEta_min', 'cluster_cell_dEta_max', 'cluster_cell_dPhi_min', 'cluster_cell_dPhi_max', 'cluster_cell_centerCellEta', 'cluster_cell_centerCellPhi', 'cluster_cell_centerCellLayer', 'cluster_cellE_norm']\n",
    "elif(source == 'jet'):\n",
    "    inputpath = path_prefix+'jets/training/'\n",
    "    rootfiles = [\"pi0\", \"piplus\"]\n",
    "    branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_ENG_CALIB_TOT']\n",
    "else:\n",
    "    assert(False)\n",
    "\n",
    "trees = {\n",
    "    rfile : ur.open(inputpath+rfile+\".root\")['ClusterTree']\n",
    "    for rfile in rootfiles\n",
    "}\n",
    "pdata = {\n",
    "    ifile : itree.pandas.df(branches, flatten=False)\n",
    "    for ifile, itree in trees.items()\n",
    "}\n",
    "\n",
    "total = 0\n",
    "for key in rootfiles:\n",
    "    total += len(pdata[key])\n",
    "\n",
    "for key in rootfiles:\n",
    "    n = len(pdata[key])\n",
    "    print(\"Number of {a:<7} events: {b:>10}\\t({c:.1f}%)\".format(a=key, b = n, c = 100. * n / total))\n",
    "print(\"Total: {}\".format(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of events for each category may be quite different -- ultimately we want to train our classifier on a \"balanced\" dataset, where we have equal numbers of entries from each category.\n",
    "\n",
    "We're training our network to classify between $\\pi^\\pm$ and $\\pi^0$ events. Thus, we should ultimately merge our $\\pi^+$ and $\\pi^-$ data.\n",
    "\n",
    "Thus, we will first generate selected indices for all categories, such that the total number of events from each category is equal, and *then* we will merge things.\n",
    "\n",
    "Note that as we're dealing with DataFrames (`pdata`) and uproot trees (`trees`, whose contents get loaded into `pcells`), we have to be careful that when we merge data, we do it the same way for both sets of objects. Otherwise we might scramble our $\\pi^\\pm$ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_indices = {}\n",
    "n_max = int(np.min(np.array([len(pdata[key]) for key in trees.keys()])))\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# If we have a piminus key, assume the dataset are piplus, piminus, pi0\n",
    "if('piminus' in trees.keys()):\n",
    "    n_indices['piplus']  = int(np.ceil((n_max / 2)))\n",
    "    n_indices['piminus'] = int(np.floor((n_max / 2)))\n",
    "    n_indices['pi0']     = n_max\n",
    "    \n",
    "# Otherwise, assume we already have piplus (or piplus + piminus) and pi0, no merging needed\n",
    "else: n_indices = {key:n_max for key in trees.keys}\n",
    "indices = {key:rng.choice(len(pdata[key]), n_indices[key], replace=False) for key in trees.keys()}\n",
    "\n",
    "# Make a boolean array version of our indices, since pandas is weird and doesn't handle non-bool indices?\n",
    "bool_indices = {}\n",
    "for key in pdata.keys():\n",
    "    bool_indices[key] = np.full(len(pdata[key]), False)\n",
    "    bool_indices[key][indices[key]] = True\n",
    "\n",
    "# Apply the (bool) indices to pdata\n",
    "for key in trees.keys():\n",
    "    pdata[key] = pdata[key][bool_indices[key]]\n",
    "\n",
    "# prepare pcells -- immediately apply our selected indices\n",
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer, indices = indices[ifile])\n",
    "        for layer in layers\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging piplus and piminus.\n"
     ]
    }
   ],
   "source": [
    "# Now with the data extracted from the trees into pcells, we merge pdata and pcells as needed.\n",
    "# Note the order in which we concatenate things: piplus -> piplus + piminus.\n",
    "if('piminus' in trees.keys()):\n",
    "    print('Merging piplus and piminus.')\n",
    "    \n",
    "    # merge pdata\n",
    "    pdata['piplus'] = pdata['piplus'].append(pdata['piminus'])\n",
    "    del pdata['piminus']\n",
    "    \n",
    "    # merge contents of pcells\n",
    "    for layer in layers:\n",
    "        pcells['piplus'][layer] = np.row_stack((pcells['piplus'][layer],pcells['piminus'][layer]))\n",
    "    del pcells['piminus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata_merged, pcells_merged, plabels = cdu.DataPrep(pdata, pcells, layers, trainfrac=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the network on $\\pi^+$ and $\\pi^0$ events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few example images.\n",
    "\n",
    "These are the images that we will use to train our network (together with a few other variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.ImagePlot(\n",
    "    pcells,\n",
    "    cluster=100,\n",
    "    layers=layers,\n",
    "    cell_shapes=cell_shapes,\n",
    "    plotpath=plotpath,\n",
    "    filename='calo_images.png',\n",
    "    plotstyle=plotstyle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_shape = (16,16)\n",
    "cpu.ImagePlot(\n",
    "    pcells,\n",
    "    cluster=100,\n",
    "    scaled_shape=scaled_shape,\n",
    "    layers=layers,\n",
    "    cell_shapes=cell_shapes,\n",
    "    plotpath=plotpath,\n",
    "    filename='calo_images_{}x{}.png'.format(*scaled_shape),\n",
    "    plotstyle=plotstyle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few histograms.\n",
    "\n",
    "These are a bit uglier than the `matplotlib` ones Max made, but it's perhaps even easier to see any differences between $\\pi^\\pm$ and $\\pi^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.gStyle.SetOptStat(0)\n",
    "\n",
    "plotstyle.SetStyle()\n",
    "\n",
    "# For storing histograms and legends, to prevent overwriting. (TODO: Probably better ways to do this in PyROOT)\n",
    "histos = []\n",
    "legends = []\n",
    "\n",
    "qtys = ['cluster_nCells', 'clusterE', 'clusterEta', 'clusterPhi', 'cluster_EM_PROBABILITY', 'cluster_sumCellE']\n",
    "qty_labels = ['Cells/Cluster', 'Cluster Energy [GeV]', 'Cluster #eta', 'Cluster #phi', 'Cluster EMProb', 'Cluster SumCellE']\n",
    "qty_ranges = [(0,500), (50,200), (-0.8,0.8), (-4.,4.), (0.,1.), (0.,2500.)]\n",
    "\n",
    "if(source == 'jet'):\n",
    "    qtys = ['cluster_nCells', 'clusterE', 'clusterEta', 'clusterPhi']\n",
    "    qty_labels = ['Cells/Cluster', 'Cluster Energy [GeV]', 'Cluster #eta', 'Cluster #phi']\n",
    "    qty_ranges = [(0,300), (0,100), (-0.8,0.8), (-4.,4.)]\n",
    "\n",
    "# Set up a canvas.\n",
    "plot_size = 500\n",
    "nx = int(np.ceil(len(qtys) / 2))\n",
    "ny = 2\n",
    "n_pad = nx * ny\n",
    "canvas = rt.TCanvas('cluster_hists','c1',plot_size * nx,plot_size * ny)\n",
    "canvas.Divide(nx,ny)\n",
    "\n",
    "colors = {'piplus':rt.kRed,'pi0':rt.kBlue}\n",
    "styles = {'piplus':3440, 'pi0':3404}\n",
    "\n",
    "n_bins=20\n",
    "for i, (qty, label, rng) in enumerate(zip(qtys, qty_labels, qty_ranges)):\n",
    "    canvas.cd(i+1)\n",
    "    leg = rt.TLegend(0.7,0.8,0.9,0.9)\n",
    "    for ptype, p in pdata.items():\n",
    "        hist = rt.TH1F('h_'+str(ptype)+'_'+str(qty),'',n_bins,rng[0],rng[1])\n",
    "        for entry in p[qty]: hist.Fill(entry)\n",
    "        integral = hist.Integral()\n",
    "        if(integral != 0): hist.Scale(1./hist.Integral())\n",
    "        hist.SetLineColor(colors[ptype])\n",
    "        hist.SetLineWidth(2)\n",
    "        hist.SetFillColorAlpha(colors[ptype],0.5)\n",
    "        hist.SetFillStyle(styles[ptype])\n",
    "        hist.Draw('HIST SAME')\n",
    "        hist.GetXaxis().SetTitle(label)\n",
    "        hist.GetYaxis().SetTitle('Normalised events')\n",
    "        hist.SetMaximum(1.5 * hist.GetMaximum())\n",
    "        leg.AddEntry(hist,pi_latex[ptype],'f')\n",
    "        leg.Draw()\n",
    "        histos.append(hist)\n",
    "        legends.append(leg)\n",
    "    if(qty in ['cluster_nCells','clusterE', 'cluster_EM_PROBABILITY', 'cluster_sumCellE']): rt.gPad.SetLogy()\n",
    "canvas.Draw()\n",
    "canvas.SaveAs(plotpath+'hist_pi0_plus_minus.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow/Keras prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "#gpu_list = [\"/gpu:0\"] #[\"/gpu:0\",\"/gpu:1\",\"/gpu:2\",\"/gpu:3\"]\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "from util.classification.models import baseline_nn_model, baseline_cnn_model, emb_cnn_model, all_cnn_model, merged_cnn_model, merged_cnn_2p_model, resnet, simple_combine_model\n",
    "from util.classification.models_exp import exp_cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare some callbacks (originally from our regression notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storing models and some of their metrics (acc, loss)\n",
    "models = {}\n",
    "model_history = {}\n",
    "model_scores = {}\n",
    "model_performance = {}\n",
    "\n",
    "# For storing info on ROC curves\n",
    "roc_fpr = {}\n",
    "roc_tpr = {}\n",
    "roc_thresh = {}\n",
    "roc_auc = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's add some info to our dictionaries that corresponds to the existing `EM LC Prob` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under curve for LC EMProb: 0.9388561940725229\n"
     ]
    }
   ],
   "source": [
    "model_key = 'LC EMProb'\n",
    "model_scores[model_key] = 1-pdata_merged[\"cluster_EM_PROBABILITY\"]\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    drawPlots=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feed-forward Neural Network\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to train simple, feed-foward neural networks -- one per calo layer. These will be our \"baseline networks\".\n",
    "\n",
    "Note that while for most of the notebook, we'll train one instance of a network per model, whereas here we will explicitly train multiple instances as we're doing one instance *per calo layer*, each for the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5\n",
    "gamma = 0.1\n",
    "min_delta = 0.0001\n",
    "patience = 3\n",
    "dropout = 0.1 # < 0 -> no dropout\n",
    "#model = baseline_nn_model(strategy, lr=lr, dropout=dropout)\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "\n",
    "model_name = 'flat'\n",
    "model_dir = modelpath + model_name # directory for loading/saving flat models\n",
    "\n",
    "for layer in layers:\n",
    "    \n",
    "    model_key = '{}_{}'.format(model_name, layer)\n",
    "    modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "    npix = cell_shapes[layer][0] * cell_shapes[layer][1]\n",
    "    models[model_key] = baseline_nn_model(strategy, npix, lr=lr, dropout=dropout)\n",
    "    \n",
    "    # train the network\n",
    "    models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "        model=models[model_key], \n",
    "        modelfile=modelfile, \n",
    "        x_train = pcells_merged[layer][pdata_merged.train], \n",
    "        y_train = plabels[pdata_merged.train], \n",
    "        x_valid = pcells_merged[layer][pdata_merged.val], \n",
    "        y_valid = plabels[pdata_merged.val], \n",
    "        callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "        epochs=nepochs, \n",
    "        batch_size=batch_size, \n",
    "        verbose=verbose, \n",
    "        overwriteModel=overwriteModel,\n",
    "        finishTraining=finishTraining\n",
    "    )\n",
    "        \n",
    "    # get performance metric from test set\n",
    "    model_performance[model_key] = models[model_key].evaluate(\n",
    "        pcells_merged[layer][pdata_merged.test],\n",
    "        plabels[pdata_merged.test],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print('Finished layer {}.'.format(layer))\n",
    "    \n",
    "    # get/recalculate network scores for the dataset\n",
    "    model_scores[model_key] = models[model_key].predict(pcells_merged[layer])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at accuracy and loss, as well as ROC curves, for each network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_flat',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Network\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a simple combination network. Its inputs will be the *outputs* of our simple, feed-forward neural networks from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'simple'\n",
    "model_dir = modelpath + model_name # directory for loading/saving simple combination network\n",
    "model_key = 'simpleCombine'\n",
    "\n",
    "model_scores_stack = np.column_stack( [model_scores['flat_{}'.format(layer)] for layer in layers])\n",
    "n_input = model_scores_stack.shape[1]\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.1\n",
    "min_delta = 0.0005\n",
    "patience = 5\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key,file_extension)\n",
    "models[model_key] = simple_combine_model(strategy, lr=lr, n_input=n_input)\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = model_scores_stack[pdata_merged.train],\n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = model_scores_stack[pdata_merged.val], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    model_scores_stack[pdata_merged.test],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(model_scores_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys = ['simpleCombine'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_simple',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'flat_EMB1', 'simpleCombine']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "Let's try some convolutional neural networks -- afterwards we'll also try an implementation of ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-calo-layer CNN's\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "First up, we can try creating a set of CNN's, where each only uses an image from a single calorimeter layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5\n",
    "gamma = 0.1\n",
    "min_delta = 0.0001\n",
    "patience = 3\n",
    "augmentation = True\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "\n",
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving flat models\n",
    "\n",
    "filters = {\n",
    "    'EMB1': (2,4), \n",
    "    'EMB2': (4,4), \n",
    "    'EMB3': (4,2), \n",
    "    'TileBar0': (2,2), \n",
    "    'TileBar1': (2,2), \n",
    "    'TileBar2': (1,1)\n",
    "}\n",
    "pools = {\n",
    "    'EMB1': (1,1), \n",
    "    'EMB2': (2,2), \n",
    "    'EMB3': (1,1), \n",
    "    'TileBar0': (1,1), \n",
    "    'TileBar1': (1,1), \n",
    "    'TileBar2': (1,1)\n",
    "}\n",
    "\n",
    "for layer in layers:\n",
    "    model_key = '{}_{}'.format(model_name, layer)\n",
    "    modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)    \n",
    "    models[model_key] = baseline_cnn_model(input_shape=cell_shapes[layer], f=filters[layer], pool=pools[layer], lr=lr, augmentation=augmentation)\n",
    "    \n",
    "    # train the network\n",
    "    models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "        model=models[model_key], \n",
    "        modelfile=modelfile, \n",
    "        x_train = rn_data['train'][layer], \n",
    "        y_train = plabels[pdata_merged.train], \n",
    "        x_valid = rn_data['valid'][layer],\n",
    "        y_valid = plabels[pdata_merged.val], \n",
    "        callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "        epochs=nepochs, \n",
    "        batch_size=batch_size, \n",
    "        verbose=verbose, \n",
    "        overwriteModel=overwriteModel,\n",
    "        finishTraining=finishTraining\n",
    "    )\n",
    "        \n",
    "    # get performance metric from test set\n",
    "    model_performance[model_key] = models[model_key].evaluate(\n",
    "        rn_data['test'][layer],\n",
    "        plabels[pdata_merged.test],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print('Finished layer {}.'.format(layer))\n",
    "    \n",
    "    # get/recalculate network scores for the dataset\n",
    "    model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened[layer])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    plotstyle=plotstyle,\n",
    "    model_keys = ['cnn_{}'.format(layer) for layer in layers]\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['cnn_{}'.format(layer) for layer in layers]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination Network: Take 2\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "Let's try another combination network, using our single layer CNN's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'simple_cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving simple combination network\n",
    "model_key = 'simpleCombine_cnn'\n",
    "\n",
    "model_scores_stack = np.column_stack([model_scores['cnn_{}'.format(layer)] for layer in layers])\n",
    "n_input = model_scores_stack.shape[1]\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.1\n",
    "min_delta = 0.0001\n",
    "patience = 3\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key,file_extension)\n",
    "models[model_key] = simple_combine_model(strategy, lr=lr, n_input=n_input)\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = model_scores_stack[pdata_merged.train],\n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = model_scores_stack[pdata_merged.val], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    model_scores_stack[pdata_merged.test],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(model_scores_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    plotstyle=plotstyle,\n",
    "    model_keys = ['simpleCombine_cnn']\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_simple_cnn',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'simpleCombine', 'simpleCombine_cnn']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks: More complicated architectures\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "We can also consider more complex CNN's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 EMB layers, separate\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "First, we can try one that uses all three EMB layers as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "# keep only the EMB layers\n",
    "pcells_merged_unflattened = {key:pcells_merged_unflattened[key] for key in ['EMB1','EMB2','EMB3']}\n",
    "\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_EMB_all'\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = emb_cnn_model(\n",
    "    lr=lr,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_EMB_all'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_EMB_all',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'simpleCombine_cnn', 'cnn_EMB_all']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All layers, separate\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "Similarly, we can try a network using all six calo layers, as separate 1-channel images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_all'\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = all_cnn_model(\n",
    "    lr=lr,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_all'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_all',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'simpleCombine_cnn', 'cnn_EMB_all', 'cnn_all']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All layers, merged\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "Of course, we can also treat the 6 calo layers as 6 channels of the *same* image. Note that this will require some rescaling of the images so that their dimensions match -- we'll also be careful to preserve their integrals. This last point is probably even more relevant for regression tasks (i.e. predicting energy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_merged'\n",
    "\n",
    "input_shape = (16,16)\n",
    "dropout = 0.2\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = merged_cnn_model(\n",
    "    lr=lr,\n",
    "    input_shape=input_shape,\n",
    "    dropout=dropout,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_merged'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_merged',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'simpleCombine_cnn', 'cnn_EMB_all', 'cnn_all', 'cnn_merged']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMB merged + TileBar merged\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "As another variation on this theme, we can try a \"merged\" CNN where we separately treat the 3 EMB layers and the 3 TileBar layers. This will allow us to perform separate rescalings for each -- and we may thus be able to have fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_merged_2p'\n",
    "\n",
    "input_shape1 = (16,16)\n",
    "input_shape2 = (4,4)\n",
    "dropout = 0.2\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = merged_cnn_2p_model(\n",
    "    lr=lr,\n",
    "    input_shape1=input_shape1,\n",
    "    input_shape2=input_shape2,\n",
    "    dropout=dropout,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_merged_2p'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_merged_2p',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'cnn_merged', 'cnn_merged_2p']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMB merged + TileBar depth\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "As an experiment, we can try a network where we use a CNN approach for the EMB layers, whereas for TileBar we just use the integrals of the 3 Tilebar layers. In other words, we don't fully use TileBar images, but just get some depth info from how populated they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=True)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")\n",
    "rn_data = {key: list(val.values()) for key,val in rn_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1483/1483 [==============================] - 11s 7ms/step - loss: 0.4262 - acc: 0.8150 - val_loss: 0.3727 - val_acc: 0.8431 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "1483/1483 [==============================] - 11s 7ms/step - loss: 0.3600 - acc: 0.8571 - val_loss: 0.3369 - val_acc: 0.8746 - lr: 9.5123e-05\n",
      "Epoch 3/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.3344 - acc: 0.8715 - val_loss: 0.3188 - val_acc: 0.8824 - lr: 9.0484e-05\n",
      "Epoch 4/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.3195 - acc: 0.8784 - val_loss: 0.3063 - val_acc: 0.8871 - lr: 8.6071e-05\n",
      "Epoch 5/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.3084 - acc: 0.8859 - val_loss: 0.2942 - val_acc: 0.8945 - lr: 8.1873e-05\n",
      "Epoch 6/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2993 - acc: 0.8915 - val_loss: 0.2875 - val_acc: 0.8958 - lr: 7.7880e-05\n",
      "Epoch 7/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2927 - acc: 0.8948 - val_loss: 0.2817 - val_acc: 0.9002 - lr: 7.4082e-05\n",
      "Epoch 8/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2876 - acc: 0.8971 - val_loss: 0.2779 - val_acc: 0.9008 - lr: 7.0469e-05\n",
      "Epoch 9/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2838 - acc: 0.8987 - val_loss: 0.2733 - val_acc: 0.9035 - lr: 6.7032e-05\n",
      "Epoch 10/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2804 - acc: 0.9004 - val_loss: 0.2706 - val_acc: 0.9040 - lr: 6.3763e-05\n",
      "Epoch 11/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2774 - acc: 0.9017 - val_loss: 0.2681 - val_acc: 0.9065 - lr: 6.0653e-05\n",
      "Epoch 12/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2753 - acc: 0.9026 - val_loss: 0.2651 - val_acc: 0.9071 - lr: 5.7695e-05\n",
      "Epoch 13/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2729 - acc: 0.9037 - val_loss: 0.2630 - val_acc: 0.9086 - lr: 5.4881e-05\n",
      "Epoch 14/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2710 - acc: 0.9043 - val_loss: 0.2613 - val_acc: 0.9067 - lr: 5.2205e-05\n",
      "Epoch 15/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2688 - acc: 0.9053 - val_loss: 0.2593 - val_acc: 0.9089 - lr: 4.9659e-05\n",
      "Epoch 16/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2669 - acc: 0.9060 - val_loss: 0.2571 - val_acc: 0.9091 - lr: 4.7237e-05\n",
      "Epoch 17/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2649 - acc: 0.9068 - val_loss: 0.2563 - val_acc: 0.9098 - lr: 4.4933e-05\n",
      "Epoch 18/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2636 - acc: 0.9074 - val_loss: 0.2548 - val_acc: 0.9100 - lr: 4.2741e-05\n",
      "Epoch 19/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2622 - acc: 0.9080 - val_loss: 0.2533 - val_acc: 0.9103 - lr: 4.0657e-05\n",
      "Epoch 20/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2612 - acc: 0.9084 - val_loss: 0.2524 - val_acc: 0.9107 - lr: 3.8674e-05\n",
      "Epoch 21/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2596 - acc: 0.9089 - val_loss: 0.2497 - val_acc: 0.9117 - lr: 3.6788e-05\n",
      "Epoch 22/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2582 - acc: 0.9095 - val_loss: 0.2491 - val_acc: 0.9113 - lr: 3.4994e-05\n",
      "Epoch 23/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2568 - acc: 0.9096 - val_loss: 0.2478 - val_acc: 0.9121 - lr: 3.3287e-05\n",
      "Epoch 24/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2561 - acc: 0.9099 - val_loss: 0.2472 - val_acc: 0.9125 - lr: 3.1664e-05\n",
      "Epoch 25/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2552 - acc: 0.9104 - val_loss: 0.2457 - val_acc: 0.9126 - lr: 3.0119e-05\n",
      "Epoch 26/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2543 - acc: 0.9105 - val_loss: 0.2452 - val_acc: 0.9129 - lr: 2.8650e-05\n",
      "Epoch 27/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2530 - acc: 0.9111 - val_loss: 0.2443 - val_acc: 0.9129 - lr: 2.7253e-05\n",
      "Epoch 28/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2527 - acc: 0.9110 - val_loss: 0.2436 - val_acc: 0.9131 - lr: 2.5924e-05\n",
      "Epoch 29/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2517 - acc: 0.9114 - val_loss: 0.2428 - val_acc: 0.9133 - lr: 2.4660e-05\n",
      "Epoch 30/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2507 - acc: 0.9114 - val_loss: 0.2422 - val_acc: 0.9137 - lr: 2.3457e-05\n",
      "Epoch 31/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2509 - acc: 0.9117 - val_loss: 0.2413 - val_acc: 0.9139 - lr: 2.2313e-05\n",
      "Epoch 32/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2496 - acc: 0.9120 - val_loss: 0.2410 - val_acc: 0.9137 - lr: 2.1225e-05\n",
      "Epoch 33/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2495 - acc: 0.9121 - val_loss: 0.2405 - val_acc: 0.9142 - lr: 2.0190e-05\n",
      "Epoch 34/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2487 - acc: 0.9125 - val_loss: 0.2399 - val_acc: 0.9145 - lr: 1.9205e-05\n",
      "Epoch 35/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2484 - acc: 0.9127 - val_loss: 0.2395 - val_acc: 0.9146 - lr: 1.8268e-05\n",
      "Epoch 36/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2482 - acc: 0.9128 - val_loss: 0.2389 - val_acc: 0.9149 - lr: 1.7377e-05\n",
      "Epoch 37/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2472 - acc: 0.9125 - val_loss: 0.2384 - val_acc: 0.9148 - lr: 1.6530e-05\n",
      "Epoch 38/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2472 - acc: 0.9128 - val_loss: 0.2380 - val_acc: 0.9150 - lr: 1.5724e-05\n",
      "Epoch 39/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2465 - acc: 0.9129 - val_loss: 0.2377 - val_acc: 0.9153 - lr: 1.4957e-05\n",
      "Epoch 40/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2460 - acc: 0.9130 - val_loss: 0.2375 - val_acc: 0.9149 - lr: 1.4227e-05\n",
      "Epoch 41/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2459 - acc: 0.9130 - val_loss: 0.2373 - val_acc: 0.9149 - lr: 1.3534e-05\n",
      "Epoch 42/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2453 - acc: 0.9131 - val_loss: 0.2367 - val_acc: 0.9156 - lr: 1.2873e-05\n",
      "Epoch 43/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2453 - acc: 0.9132 - val_loss: 0.2366 - val_acc: 0.9153 - lr: 1.2246e-05\n",
      "Epoch 44/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2454 - acc: 0.9132 - val_loss: 0.2364 - val_acc: 0.9154 - lr: 1.1648e-05\n",
      "Epoch 45/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2449 - acc: 0.9136 - val_loss: 0.2359 - val_acc: 0.9155 - lr: 1.1080e-05\n",
      "Epoch 46/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2443 - acc: 0.9136 - val_loss: 0.2355 - val_acc: 0.9157 - lr: 1.0540e-05\n",
      "Epoch 47/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2441 - acc: 0.9136 - val_loss: 0.2355 - val_acc: 0.9156 - lr: 1.0026e-05\n",
      "Epoch 48/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2437 - acc: 0.9139 - val_loss: 0.2353 - val_acc: 0.9158 - lr: 9.5369e-06\n",
      "Epoch 49/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2438 - acc: 0.9139 - val_loss: 0.2349 - val_acc: 0.9159 - lr: 9.0718e-06\n",
      "Epoch 50/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2436 - acc: 0.9138 - val_loss: 0.2348 - val_acc: 0.9160 - lr: 8.6293e-06\n",
      "Epoch 51/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2430 - acc: 0.9141 - val_loss: 0.2347 - val_acc: 0.9159 - lr: 8.2085e-06\n",
      "Epoch 52/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2429 - acc: 0.9143 - val_loss: 0.2347 - val_acc: 0.9160 - lr: 7.8082e-06\n",
      "Epoch 53/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2434 - acc: 0.9139 - val_loss: 0.2341 - val_acc: 0.9160 - lr: 7.4273e-06\n",
      "Epoch 54/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2425 - acc: 0.9143 - val_loss: 0.2339 - val_acc: 0.9164 - lr: 7.0651e-06\n",
      "Epoch 55/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2421 - acc: 0.9142 - val_loss: 0.2339 - val_acc: 0.9162 - lr: 6.7205e-06\n",
      "Epoch 56/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2425 - acc: 0.9145 - val_loss: 0.2337 - val_acc: 0.9162 - lr: 6.3928e-06\n",
      "Epoch 57/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2423 - acc: 0.9142 - val_loss: 0.2335 - val_acc: 0.9164 - lr: 6.0810e-06\n",
      "Epoch 58/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2418 - acc: 0.9146 - val_loss: 0.2335 - val_acc: 0.9165 - lr: 5.7844e-06\n",
      "Epoch 59/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2422 - acc: 0.9140 - val_loss: 0.2334 - val_acc: 0.9165 - lr: 5.5023e-06\n",
      "Epoch 60/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2422 - acc: 0.9142 - val_loss: 0.2333 - val_acc: 0.9161 - lr: 5.2340e-06\n",
      "Epoch 61/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2415 - acc: 0.9148 - val_loss: 0.2330 - val_acc: 0.9166 - lr: 4.9787e-06\n",
      "Epoch 62/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2416 - acc: 0.9143 - val_loss: 0.2330 - val_acc: 0.9163 - lr: 4.7359e-06\n",
      "Epoch 63/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2416 - acc: 0.9146 - val_loss: 0.2332 - val_acc: 0.9165 - lr: 4.5049e-06\n",
      "Epoch 64/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2411 - acc: 0.9147 - val_loss: 0.2328 - val_acc: 0.9167 - lr: 4.2852e-06\n",
      "Epoch 65/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2418 - acc: 0.9146 - val_loss: 0.2326 - val_acc: 0.9167 - lr: 4.0762e-06\n",
      "Epoch 66/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2413 - acc: 0.9144 - val_loss: 0.2328 - val_acc: 0.9166 - lr: 3.8774e-06\n",
      "Epoch 67/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2412 - acc: 0.9147 - val_loss: 0.2325 - val_acc: 0.9167 - lr: 3.6883e-06\n",
      "Epoch 68/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2412 - acc: 0.9146 - val_loss: 0.2326 - val_acc: 0.9168 - lr: 3.5084e-06\n",
      "Epoch 69/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2411 - acc: 0.9149 - val_loss: 0.2323 - val_acc: 0.9168 - lr: 3.3373e-06\n",
      "Epoch 70/100\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2408 - acc: 0.9148 - val_loss: 0.2322 - val_acc: 0.9167 - lr: 3.1746e-06\n",
      "Epoch 71/100\n",
      "1483/1483 [==============================] - ETA: 0s - loss: 0.2406 - acc: 0.9146Restoring model weights from the end of the best epoch.\n",
      "1483/1483 [==============================] - 10s 7ms/step - loss: 0.2406 - acc: 0.9146 - val_loss: 0.2323 - val_acc: 0.9166 - lr: 3.0197e-06\n",
      "Epoch 00071: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'cnn_exp'\n",
    "\n",
    "input_shape = (16,16)\n",
    "dropout = 0.2\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 10\n",
    "augmentation=True\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = exp_cnn_model(\n",
    "    lr=lr,\n",
    "    input_shape=input_shape,\n",
    "    dropout=dropout,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['cnn_exp'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_cnn_exp',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'cnn_merged', 'cnn_merged_2p', 'cnn_exp']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "We can also train an instance of ResNet. As with our multi-layer CNN's, we'll need to perform some up/downscaling of images, so that they all have the same dimensions. As we did for some of our CNN's, we will use the maximum possible granularity, `(128,16)` (corresponding with maximum eta and phi dimensions among all calo layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcells_merged_unflattened = cdu.ReshapeImages(pcells_merged, cell_shapes, use_layer_names=False)\n",
    "rn_data = cdu.DictionarySplit(\n",
    "    pcells_merged_unflattened, \n",
    "    train_indices=pdata_merged.train,\n",
    "    validation_indices = pdata_merged.val,\n",
    "    test_indices = pdata_merged.test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a quick test to make sure that our image scaling is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test=False\n",
    "\n",
    "from util.keras.layers import ImageScaleBlock\n",
    "def TestImage(idxs):\n",
    "    images = [np.stack([rn_train['input{}'.format(x)][idx,:,:] for idx in idxs],axis=0) for x in range(6)]    \n",
    "    # need to add the last dimension, corresponding with channel (will be of size 1)\n",
    "    images = [np.expand_dims(im, axis=-1) for im in images]\n",
    "    return images\n",
    "\n",
    "if(image_test):\n",
    "    test_idx = np.arange(5)\n",
    "    image = TestImage(test_idx)\n",
    "    scaled_image = ImageScaleBlock((128,16),normalization=True)(image).numpy()\n",
    "    for i,im in enumerate(image):\n",
    "        integrals_old = np.sum(im,axis=(1,2)).flatten()\n",
    "        integrals_new = np.sum(scaled_image[:,:,:,i], axis=(1,2)).flatten()\n",
    "\n",
    "        ratio = integrals_old.copy()\n",
    "        ratio[ratio==0.] = 1.\n",
    "        ratio = integrals_new/ratio\n",
    "\n",
    "        integrals_old = '\\t\\t'.join(['{:.1e}'.format(x) for x in integrals_old])\n",
    "        integrals_new = '\\t\\t'.join(['{:.1e}'.format(x) for x in integrals_new])\n",
    "        ratio         = '\\t\\t'.join(['{:.1e}'.format(x) for x in ratio        ])\n",
    "\n",
    "        print('Integral {} = {}'.format(i,integrals_old))\n",
    "        print('           = {}'.format(integrals_new))\n",
    "        print('Ratios     = {}'.format(ratio))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_image_data_format('channels_last')\n",
    "model_name = 'resnet'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'resnet'\n",
    "\n",
    "lr = 1e-4\n",
    "gamma = 0.05\n",
    "min_delta = 0.001\n",
    "patience = 20 # large patience since loss sometimes fluctuates upwards for a bit?\n",
    "input_shape = (128,16)\n",
    "channels = 6\n",
    "augmentation=True\n",
    "normalization=True\n",
    "filter_sets = [\n",
    "    [64,64,256],\n",
    "    [128,128,512]\n",
    "    #[256,256,1024],\n",
    "    #[512,512,2048]\n",
    "]         \n",
    "f_vals = [3,3] # [3,3,3,3] sizes of filters in middle of conv/identity blocks\n",
    "s_vals = [1,2] # [1,2,2,2] strides for each convolutional block\n",
    "i_vals = [2,3] # [2,3,5,2] number of identity blocks per stage\n",
    "\n",
    "nepochs = 200\n",
    "batch_size = 50 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = model = resnet(\n",
    "    filter_sets=filter_sets, \n",
    "    lr=lr, \n",
    "    channels=channels, \n",
    "    f_vals=f_vals, \n",
    "    s_vals=s_vals, \n",
    "    i_vals=i_vals, \n",
    "    input_shape=input_shape, \n",
    "    augmentation=augmentation,\n",
    "    normalization=normalization\n",
    ")\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = rn_data['train'], \n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = rn_data['valid'], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    rn_data['test'],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(pcells_merged_unflattened)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys=['resnet'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_resnet',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys=['LC EMProb', 'flat_EMB1', 'resnet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Network: Take 3\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>\n",
    "For fun, we can also train a combination network where we use our flat DNN's, and ResNet as an additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_stack = np.column_stack( [model_scores['flat_{}'.format(layer)] for layer in layers])\n",
    "model_scores_stack = np.column_stack((model_scores_stack, model_scores['resnet']))\n",
    "n_input = model_scores_stack.shape[1]\n",
    "\n",
    "nepochs = 50\n",
    "batch_size = 200*ngpu\n",
    "verbose = 2\n",
    "\n",
    "\n",
    "model_name = 'simple_resnet'\n",
    "model_dir = modelpath + model_name # directory for loading/saving ResNet\n",
    "model_key = 'simpleCombine_resnet'\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.1\n",
    "min_delta = 0.001\n",
    "patience = 5\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "    \n",
    "modelfile = '{}/{}{}'.format(model_dir,model_key, file_extension)\n",
    "models[model_key] = simple_combine_model(strategy, lr=lr, n_input=n_input)\n",
    "    \n",
    "# train the network\n",
    "models[model_key], model_history[model_key] = ctu.TrainNetwork(\n",
    "    model=models[model_key], \n",
    "    modelfile=modelfile, \n",
    "    x_train = model_scores_stack[pdata_merged.train],\n",
    "    y_train = plabels[pdata_merged.train], \n",
    "    x_valid = model_scores_stack[pdata_merged.val], \n",
    "    y_valid = plabels[pdata_merged.val], \n",
    "    callbacks = GetCallbacks(modelfile, append=True, use_decay=True, gamma=gamma, min_delta=min_delta, patience=patience), \n",
    "    epochs=nepochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose, \n",
    "    overwriteModel=overwriteModel,\n",
    "    finishTraining=finishTraining\n",
    ")\n",
    "        \n",
    "# get performance metric from test set\n",
    "model_performance[model_key] = models[model_key].evaluate(\n",
    "    model_scores_stack[pdata_merged.test],\n",
    "    plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get/recalculate network scores for the dataset\n",
    "model_scores[model_key] = models[model_key].predict(model_scores_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.MetricPlot(\n",
    "    model_history,\n",
    "    plotpath=plotpath,\n",
    "    model_keys = ['simpleCombine_resnet'],\n",
    "    plotstyle=plotstyle\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_simple_resnet',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'simpleCombine', 'resnet', 'simpleCombine_cnn', 'simpleCombine_resnet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Plots\n",
    "<div style=\"text-align: right\"> <a href=\"#Quick-Navigation:\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've trained networks to our hearts' content, we can make a set of ROC curves to summarize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_summary',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'flat_EMB1', 'flat_EMB2', 'flat_EMB3', 'cnn_EMB1', 'cnn_EMB2', 'cnn_EMB3', 'simpleCombine', 'simpleCombine_cnn', 'cnn_EMB_all', 'cnn_all', 'cnn_merged', 'resnet', 'simpleCombine_resnet']\n",
    ")\n",
    "\n",
    "cpu.RocCurves(\n",
    "    model_scores,\n",
    "    data_labels=plabels[:,1],\n",
    "    indices=pdata_merged.test, \n",
    "    roc_fpr=roc_fpr, \n",
    "    roc_tpr=roc_tpr, \n",
    "    roc_thresh=roc_thresh, \n",
    "    roc_auc=roc_auc, \n",
    "    plotpath=plotpath,\n",
    "    plotname='ROC_best',\n",
    "    drawPlots=True,\n",
    "    plotstyle=plotstyle,\n",
    "    figsize=(30,10),\n",
    "    model_keys = ['LC EMProb', 'flat_EMB1', 'simpleCombine_cnn', 'simpleCombine', 'cnn_EMB_all', 'cnn_all', 'cnn_merged', 'resnet', 'simpleCombine_resnet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make a scatterplot of number of learnable parameters versus accuracy. The number of parameters can be thought of as a stand-in for complexity (though I assume that computational complexity/cost will also depend on the exact operations being performed).\n",
    "\n",
    "Note that there is some inherent hard-coding being done here, since some of our models (e.g. `simpleCombine`) use other models as input -- and for such models, we want to count their inputs' weights plus their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "complexity_scatter = {}\n",
    "\n",
    "for key, model in models.items():\n",
    "    trainable_count = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "    accuracy        = model_performance[key][1]\n",
    "    auc             = roc_auc[key]\n",
    "    complexity_scatter[key] = [trainable_count, accuracy, auc]\n",
    "    \n",
    "# manually adjust complexity_scatter for our combination networks\n",
    "\n",
    "flat_weights = np.sum([np.sum([K.count_params(w) for w in models[key].trainable_weights]) for key in ['flat_{}'.format(layer) for layer in layers]])\n",
    "cnn_weights = np.sum([np.sum([K.count_params(w) for w in models[key].trainable_weights]) for key in ['cnn_{}'.format(layer) for layer in layers]])\n",
    "\n",
    "complexity_scatter['simpleCombine'][0] += flat_weights\n",
    "complexity_scatter['simpleCombine_cnn'][0] += cnn_weights\n",
    "complexity_scatter['simpleCombine_resnet'][0] += flat_weights + complexity_scatter['resnet'][0]\n",
    "    \n",
    "fig, ax = plt.subplots(1,2, figsize=(25,10))\n",
    "for axis in ax: plotstyle.SetStylePlt(axis)\n",
    "\n",
    "# Make a scatter -- there will be a lot of points, so consider not drawing them for each network.\n",
    "draw_keys = ['flat_EMB1', 'simpleCombine_cnn', 'simpleCombine', 'cnn_EMB_all', 'cnn_all', 'cnn_merged', 'resnet', 'simpleCombine_resnet']\n",
    "draw_cols = [plotstyle.colors[(i+1) % (len(plotstyle.colors)-1)] for i in range(len(draw_keys))]\n",
    "\n",
    "for i,key in enumerate(draw_keys):\n",
    "    if(key not in complexity_scatter.keys()): continue\n",
    "    x,y1,y2 = complexity_scatter[key]\n",
    "    ax[0].scatter(x=x,y=y1, label=key, color=draw_cols[i], s=64)\n",
    "    ax[1].scatter(x=x,y=y2, label=key, color=draw_cols[i], s=64)\n",
    "    \n",
    "    ax[0].set_xlabel('# Learnable Parameters')\n",
    "    ax[1].set_xlabel('# Learnable Parameters')\n",
    "    \n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[1].set_ylabel('Area under ROC curve')\n",
    "    \n",
    "    ax[0].set_xlim([0.,2.5e6])\n",
    "    ax[1].set_xlim([0.,2.5e6])\n",
    "\n",
    "legs = []\n",
    "for i,axis in enumerate(ax):\n",
    "    legs.append(axis.legend(facecolor=plotstyle.canv_plt))\n",
    "    for leg_text in legs[-1].get_texts(): leg_text.set_color(plotstyle.text_plt)\n",
    "        \n",
    "qu.SaveSubplots(fig, ax, ['acc_params', 'auc_params'], savedir=plotpath, ps=plotstyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots, `cnn_all` looks like a relatively appealing option -- it's one of the highest-performing networks, and it actually has the fewest learnable parameters. `resnet` performs slightly better, but at the cost of many more learnable weights. This added complexity may translate to slower deployment time, and cause issues if we try to use this network in a triggering context (i.e. on an FPGA), where we will have to deal with significant hardware and timing constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to also make a table (or plot) depicting the false positive rate at different true positive rate (i.e. pick out a few reference points from the ROC curves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "fpr_data = {}\n",
    "tpr_vals = np.array([0.6,0.8,0.9,0.95])\n",
    "\n",
    "for tpr_val in tpr_vals: \n",
    "    fpr_data[tpr_val] = {}\n",
    "    for i, draw_key in enumerate(draw_keys):\n",
    "        \n",
    "        # Want to find the fpr at the given tpr\n",
    "        fpr = roc_fpr[draw_key]\n",
    "        tpr = roc_tpr[draw_key]\n",
    "        f = interp1d(tpr, fpr)        \n",
    "        fpr_data[tpr_val][draw_key] = f(tpr_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plotstyle.SetStylePlt(ax)\n",
    "\n",
    "# multiple bars -- handle visual offsets\n",
    "bar_width = 0.2\n",
    "n = len(tpr_vals)\n",
    "offsets = np.linspace(0.,(n-1) * bar_width,n)\n",
    "offsets = offsets - np.mean(offsets)\n",
    "\n",
    "# hatches for distinguishing between different tpr values\n",
    "hatches = ['/','\\\\','//','\\\\\\\\']\n",
    "\n",
    "x = np.arange(len(draw_keys))\n",
    "rects = []\n",
    "for i,tpr_val in enumerate(fpr_data.keys()):\n",
    "    bar = ax.bar(\n",
    "        x + offsets[i], \n",
    "        fpr_data[tpr_val].values(),\n",
    "        bar_width,\n",
    "        label='@ True positive rate = {:.2f}'.format(tpr_val), \n",
    "        color=draw_cols,\n",
    "        hatch = hatches[i],\n",
    "        edgecolor=plotstyle.text_plt\n",
    "    )\n",
    "    \n",
    "    for rect in bar:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{:.2f}'.format(height),\n",
    "            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "            xytext=(0, 3),  # 3 points vertical offset\n",
    "            textcoords=\"offset points\",\n",
    "            color=plotstyle.text_plt,\n",
    "            ha='center', va='bottom'\n",
    "                   )\n",
    "    \n",
    "    rects.append(bar)\n",
    "\n",
    "result = ax.set_xticks(x)\n",
    "result = ax.set_xticklabels(draw_keys)\n",
    "\n",
    "ax.set_xlabel('Network')\n",
    "ax.set_ylabel('False positive rate')\n",
    "\n",
    "leg = ax.legend(facecolor=plotstyle.canv_plt)\n",
    "for leg_text in leg.get_texts(): leg_text.set_color(plotstyle.text_plt)\n",
    "    \n",
    "qu.SaveSubplots(fig, np.array([ax]), ['fpr'], savedir=plotpath, ps=plotstyle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
